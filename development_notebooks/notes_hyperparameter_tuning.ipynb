{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "598e044b",
   "metadata": {},
   "source": [
    "This notebook will attempt to find optimal hyperparameters for XGBoost applied on pneumonia, chf and aspiration risk factor annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce2615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext cudf.pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom display of tables for easier inspection\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce4cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook-specific imports\n",
    "from custom_functions import tokenizer_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd12b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data locations\n",
    "basedir = Path(\"../..\")\n",
    "analysis_location = basedir / 'Analysis_Data'\n",
    "cohort = 'hospital_a_2013'\n",
    "path = analysis_location / cohort\n",
    "\n",
    "# Figures\n",
    "figure_path = basedir / \"Figures\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9aaf796",
   "metadata": {},
   "source": [
    "### File reading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13864933",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_annot = pd.read_csv(path / \"attending_notes_annotated.csv\")\n",
    "notes_annot['notes_timestamp'] = pd.to_timedelta(notes_annot['notes_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185cf3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting tables to only have entries that matched, and relevant columns\n",
    "pneumonia_notes = notes_annot.loc[\n",
    "    notes_annot['seg_pneumonia'] != \"Invalid\",\n",
    "    [\n",
    "        'encounter_id',\n",
    "        'notes_timestamp',\n",
    "        'notes_text',\n",
    "        'pneumonia',\n",
    "        'pneumonia_sw',\n",
    "        'seg_pneumonia'\n",
    "    ]\n",
    "]\n",
    "pneumonia_notes['seg_pneumonia'] = pneumonia_notes['seg_pneumonia'].str.replace(r\"'\", r\"\", regex=True)\n",
    "pneumonia_notes['seg_pneumonia'] = pneumonia_notes['seg_pneumonia'].str.replace(r\"\\[\", r\"\", regex=True)\n",
    "pneumonia_notes['seg_pneumonia'] = pneumonia_notes['seg_pneumonia'].str.replace(r\"\\]\", r\"\", regex=True)\n",
    "pneumonia_notes['seg_pneumonia'] = pneumonia_notes['seg_pneumonia'].str.replace(r\",\", r\"\", regex=True)\n",
    "\n",
    "\n",
    "chf_notes = notes_annot.loc[\n",
    "    notes_annot['seg_chf'] != \"Invalid\",\n",
    "    [\n",
    "        'encounter_id',\n",
    "        'notes_timestamp',\n",
    "        \"notes_text\",\n",
    "        'chf',\n",
    "        'chf_sw',\n",
    "        'seg_chf'\n",
    "    ]\n",
    "]\n",
    "chf_notes['seg_chf'] = chf_notes['seg_chf'].str.replace(r\"'\", r\"\", regex=True)\n",
    "chf_notes['seg_chf'] = chf_notes['seg_chf'].str.replace(r\"\\[\", r\"\", regex=True)\n",
    "chf_notes['seg_chf'] = chf_notes['seg_chf'].str.replace(r\"\\]\", r\"\", regex=True)\n",
    "chf_notes['seg_chf'] = chf_notes['seg_chf'].str.replace(r\",\", r\"\", regex=True)\n",
    "\n",
    "\n",
    "aspiration_notes = notes_annot.loc[\n",
    "    notes_annot['seg_aspiration'] != \"Invalid\",\n",
    "    [\n",
    "        'encounter_id',\n",
    "        'notes_timestamp',\n",
    "        'notes_text',\n",
    "        'aspiration',\n",
    "        'aspiration_sw',\n",
    "        'seg_aspiration'\n",
    "    ]\n",
    "]\n",
    "aspiration_notes['seg_aspiration'] = aspiration_notes['seg_aspiration'].str.replace(r\"'\", r\"\", regex=True)\n",
    "aspiration_notes['seg_aspiration'] = aspiration_notes['seg_aspiration'].str.replace(r\"\\[\", r\"\", regex=True)\n",
    "aspiration_notes['seg_aspiration'] = aspiration_notes['seg_aspiration'].str.replace(r\"\\]\", r\"\", regex=True)\n",
    "aspiration_notes['seg_aspiration'] = aspiration_notes['seg_aspiration'].str.replace(r\",\", r\"\", regex=True)\n",
    "\n",
    "\n",
    "sepsis_notes = notes_annot.loc[\n",
    "    notes_annot['seg_sepsis'] != \"Invalid\",\n",
    "    [\n",
    "        'encounter_id',\n",
    "        'notes_timestamp',\n",
    "        'notes_text',\n",
    "        'sepsis',\n",
    "        'sepsis_sw',\n",
    "        'seg_sepsis'\n",
    "    ]\n",
    "]\n",
    "sepsis_notes['seg_sepsis'] = sepsis_notes['seg_sepsis'].str.replace(r\"'\", r\"\", regex=True)\n",
    "sepsis_notes['seg_sepsis'] = sepsis_notes['seg_sepsis'].str.replace(r\"\\[\", r\"\", regex=True)\n",
    "sepsis_notes['seg_sepsis'] = sepsis_notes['seg_sepsis'].str.replace(r\"\\]\", r\"\", regex=True)\n",
    "sepsis_notes['seg_sepsis'] = sepsis_notes['seg_sepsis'].str.replace(r\",\", r\"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7fe726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing null SW adjudications as zero\n",
    "pneumonia_notes['pneumonia_sw'] = pneumonia_notes['pneumonia_sw'].fillna(0)\n",
    "chf_notes['chf_sw'] = chf_notes['chf_sw'].fillna(0)\n",
    "aspiration_notes['aspiration_sw'] = aspiration_notes['aspiration_sw'].fillna(0)\n",
    "sepsis_notes['sepsis_sw'] = sepsis_notes['sepsis_sw'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c01d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting encounters from each of the datasets\n",
    "pneumonia_enctrs = pneumonia_notes['encounter_id'].unique()\n",
    "chf_enctrs = chf_notes['encounter_id'].unique()\n",
    "aspiration_enctrs = aspiration_notes['encounter_id'].unique()\n",
    "sepsis_enctrs = sepsis_notes['encounter_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b800d",
   "metadata": {},
   "source": [
    "### Importing machine learning libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b83f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models/algorithms/classifiers\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Performance metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, brier_score_loss\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from hyperopt.pyll.base import scope \n",
    "\n",
    "# Text vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016abdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cv_func(hyperparams, df, encounters, train_col, label_col, train_index, test_index, score='log_loss'):\n",
    "    train_encounters = encounters[train_index]\n",
    "    test_encounters = encounters[test_index]\n",
    "    \n",
    "    train = df['encounter_id'].isin(train_encounters)\n",
    "    test = df['encounter_id'].isin(test_encounters)\n",
    "    \n",
    "    X_train = df.loc[train, train_col]\n",
    "    X_test = df.loc[test, train_col]\n",
    "    Y_train = df.loc[train, label_col]\n",
    "    Y_test = df.loc[test, label_col]\n",
    "    \n",
    "    #vectorize\n",
    "    vect = CountVectorizer(\n",
    "        tokenizer=tokenizer_better,\n",
    "        ngram_range=(1,2),\n",
    "        max_features=200\n",
    "        )\n",
    "    \n",
    "    vect.fit(X_train)\n",
    "    X_train_vect = vect.transform(X_train)\n",
    "    X_test_vect = vect.transform(X_test)\n",
    "    \n",
    "    model = XGBClassifier(\n",
    "        base_score=hyperparams['base_score'],\n",
    "        n_estimators=hyperparams['n_estimators'],\n",
    "        max_depth=hyperparams['max_depth'],\n",
    "        learning_rate=hyperparams['learning_rate'],\n",
    "        gamma=hyperparams['gamma'],\n",
    "        min_child_weight=hyperparams['min_child_weight'],\n",
    "        max_delta_step=hyperparams['max_delta_step'],\n",
    "        subsample=hyperparams['subsample'],\n",
    "        random_state=0,\n",
    "        tree_method='hist'\n",
    "        )\n",
    "    \n",
    "    model.fit(X_train_vect, Y_train)\n",
    "    \n",
    "    test_preds = model.predict_proba(X_test_vect)[:,1]\n",
    "    \n",
    "    if score=='auc':\n",
    "        test_score = roc_auc_score(Y_test, test_preds)\n",
    "    elif score=='brier':\n",
    "        test_score = brier_score_loss(Y_test, test_preds)\n",
    "    elif score=='log_loss':\n",
    "        test_score = log_loss(Y_test, test_preds)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid scoring scheme, enter either 'auc', 'logloss', or 'brier'\")\n",
    "        \n",
    "    return test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6345b7e",
   "metadata": {},
   "source": [
    "### Evaluating performance of models with default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca336cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_train_pna = []\n",
    "auc_test_pna = []\n",
    "brier_train_pna = []\n",
    "brier_test_pna = []\n",
    "\n",
    "auc_train_chf = []\n",
    "auc_test_chf = []\n",
    "brier_train_chf = []\n",
    "brier_test_chf = []\n",
    "\n",
    "auc_train_aspiration = []\n",
    "auc_test_aspiration = []\n",
    "brier_train_aspiration = []\n",
    "brier_test_aspiration = []\n",
    "\n",
    "auc_train_sepsis = []\n",
    "auc_test_sepsis = []\n",
    "brier_train_sepsis = []\n",
    "brier_test_sepsis = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1aec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pna = pneumonia_notes['seg_pneumonia']\n",
    "Y_pna = pneumonia_notes['pneumonia_sw']\n",
    "\n",
    "X_chf = chf_notes['seg_chf']\n",
    "Y_chf = chf_notes['chf_sw']\n",
    "\n",
    "X_aspiration = aspiration_notes['seg_aspiration']\n",
    "Y_aspiration = aspiration_notes['aspiration_sw']\n",
    "\n",
    "X_sepsis = sepsis_notes['seg_sepsis']\n",
    "Y_sepsis = sepsis_notes['sepsis_sw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77beff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default is to do 5-fold CV\n",
    "cv = KFold()\n",
    "list_shap_values = list()\n",
    "list_test_sets = list()\n",
    "\n",
    "# We are not splitting by sample, but by encounter\n",
    "for train_index, test_index in cv.split(pneumonia_enctrs):\n",
    "    train_encounters = pneumonia_enctrs[train_index]\n",
    "    test_encounters = pneumonia_enctrs[test_index]\n",
    "    \n",
    "    train = pneumonia_notes['encounter_id'].isin(train_encounters)\n",
    "    test = pneumonia_notes['encounter_id'].isin(test_encounters)\n",
    "    \n",
    "    X_train, X_test = (\n",
    "        pneumonia_notes.loc[train, \"seg_pneumonia\"].values,\n",
    "        pneumonia_notes.loc[test, \"seg_pneumonia\"].values\n",
    "        )\n",
    "    Y_train, Y_test = (\n",
    "        pneumonia_notes.loc[train, \"pneumonia_sw\"].values,\n",
    "        pneumonia_notes.loc[test, \"pneumonia_sw\"].values\n",
    "        )\n",
    "    \n",
    "    # Vectorize\n",
    "    vect_pna = CountVectorizer(\n",
    "        tokenizer=tokenizer_better,\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=200\n",
    "        )\n",
    "\n",
    "    vect_pna.fit(X_train)\n",
    "    X_train_vect = vect_pna.transform(X_train).toarray()\n",
    "    X_test_vect = vect_pna.transform(X_test).toarray()\n",
    "    features = {value: key for key, value in vect_pna.vocabulary_.items()}\n",
    "    \n",
    "    # Train model\n",
    "    pna_model = XGBClassifier(random_state=0, device='cpu', tree_method='hist')\n",
    "    pna_model.fit(X_train_vect, Y_train)\n",
    "    \n",
    "    # Predictions    \n",
    "    pna_train_preds = pna_model.predict_proba(X_train_vect)[:,1]\n",
    "    pna_test_preds = pna_model.predict_proba(X_test_vect)[:,1]\n",
    "    \n",
    "    # Gathering AUCs and Brier scores    \n",
    "    auc_train_pna.append(roc_auc_score(Y_train, pna_train_preds))\n",
    "    auc_test_pna.append(roc_auc_score(Y_test, pna_test_preds))\n",
    "    brier_train_pna.append(brier_score_loss(Y_train, pna_train_preds))\n",
    "    brier_test_pna.append(brier_score_loss(Y_test, pna_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold()\n",
    "\n",
    "for train_index, test_index in cv.split(chf_enctrs):\n",
    "    train_encounters = chf_enctrs[train_index]\n",
    "    test_encounters = chf_enctrs[test_index]\n",
    "    \n",
    "    train = chf_notes['encounter_id'].isin(train_encounters)\n",
    "    test = chf_notes['encounter_id'].isin(test_encounters)\n",
    "    \n",
    "    X_train, X_test = (\n",
    "        chf_notes.loc[train, \"seg_chf\"].values,\n",
    "        chf_notes.loc[test, \"seg_chf\"].values\n",
    "        )\n",
    "    Y_train, Y_test = (\n",
    "        chf_notes.loc[train, \"chf_sw\"].values,\n",
    "        chf_notes.loc[test, \"chf_sw\"].values\n",
    "        )\n",
    "    \n",
    "    # Vectorize\n",
    "    vect_chf = CountVectorizer(\n",
    "        tokenizer=tokenizer_better,\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=200\n",
    "        )\n",
    "\n",
    "    vect_chf.fit(X_train)\n",
    "    X_train_vect = vect_chf.transform(X_train).toarray()\n",
    "    X_test_vect = vect_chf.transform(X_test).toarray()\n",
    "    features = {value: key for key, value in vect_chf.vocabulary_.items()}\n",
    "    \n",
    "    # Train model\n",
    "    chf_model = XGBClassifier(random_state=0)\n",
    "    chf_model.fit(X_train_vect, Y_train)\n",
    "    \n",
    "    # Predictions    \n",
    "    chf_train_preds = chf_model.predict_proba(X_train_vect)[:,1]\n",
    "    chf_test_preds = chf_model.predict_proba(X_test_vect)[:,1]\n",
    "    \n",
    "    # Gathering AUCs and Brier scores    \n",
    "    auc_train_chf.append(roc_auc_score(Y_train, chf_train_preds))\n",
    "    auc_test_chf.append(roc_auc_score(Y_test, chf_test_preds))\n",
    "    brier_train_chf.append(brier_score_loss(Y_train, chf_train_preds))\n",
    "    brier_test_chf.append(brier_score_loss(Y_test, chf_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c612f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold()\n",
    "\n",
    "for train_index, test_index in cv.split(aspiration_enctrs):\n",
    "    train_encounters = aspiration_enctrs[train_index]\n",
    "    test_encounters = aspiration_enctrs[test_index]\n",
    "    \n",
    "    train = aspiration_notes['encounter_id'].isin(train_encounters)\n",
    "    test = aspiration_notes['encounter_id'].isin(test_encounters)\n",
    "    \n",
    "    X_train, X_test = (\n",
    "        aspiration_notes.loc[train, \"seg_aspiration\"].values,\n",
    "        aspiration_notes.loc[test, \"seg_aspiration\"].values\n",
    "        )\n",
    "    Y_train, Y_test = (\n",
    "        aspiration_notes.loc[train, \"aspiration_sw\"].values,\n",
    "        aspiration_notes.loc[test, \"aspiration_sw\"].values\n",
    "        )\n",
    "    \n",
    "    # Vectorize\n",
    "    vect_aspiration = CountVectorizer(\n",
    "        tokenizer=tokenizer_better,\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=200\n",
    "        )\n",
    "\n",
    "    vect_aspiration.fit(X_train)\n",
    "    X_train_vect = vect_aspiration.transform(X_train).toarray()\n",
    "    X_test_vect = vect_aspiration.transform(X_test).toarray()\n",
    "    features = {value: key for key, value in vect_aspiration.vocabulary_.items()}\n",
    "    \n",
    "    # Train model\n",
    "    aspiration_model = XGBClassifier(random_state=0)\n",
    "    aspiration_model.fit(X_train_vect, Y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    aspiration_train_preds = aspiration_model.predict_proba(X_train_vect)[:,1]\n",
    "    aspiration_test_preds = aspiration_model.predict_proba(X_test_vect)[:,1]\n",
    "    \n",
    "    # Gathering AUCs and Brier scores\n",
    "    auc_train_aspiration.append(roc_auc_score(Y_train, aspiration_train_preds))\n",
    "    auc_test_aspiration.append(roc_auc_score(Y_test, aspiration_test_preds))\n",
    "    brier_train_aspiration.append(brier_score_loss(Y_train, aspiration_train_preds))\n",
    "    brier_test_aspiration.append(brier_score_loss(Y_test, aspiration_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc8a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold()\n",
    "\n",
    "for train_index, test_index in cv.split(sepsis_enctrs):\n",
    "    train_encounters = sepsis_enctrs[train_index]\n",
    "    test_encounters = sepsis_enctrs[test_index]\n",
    "    \n",
    "    train = sepsis_notes['encounter_id'].isin(train_encounters)\n",
    "    test = sepsis_notes['encounter_id'].isin(test_encounters)\n",
    "    \n",
    "    X_train, X_test = (\n",
    "        sepsis_notes.loc[train, \"seg_sepsis\"].values,\n",
    "        sepsis_notes.loc[test, \"seg_sepsis\"].values\n",
    "        )\n",
    "    Y_train, Y_test = (\n",
    "        sepsis_notes.loc[train, \"sepsis_sw\"].values,\n",
    "        sepsis_notes.loc[test, \"sepsis_sw\"].values\n",
    "        )\n",
    "    \n",
    "    # Vectorize\n",
    "    vect_sepsis = CountVectorizer(\n",
    "        tokenizer=tokenizer_better,\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=200\n",
    "        )\n",
    "\n",
    "    vect_sepsis.fit(X_train)\n",
    "    X_train_vect = vect_sepsis.transform(X_train).toarray()\n",
    "    X_test_vect = vect_sepsis.transform(X_test).toarray()\n",
    "    features = {value: key for key, value in vect_sepsis.vocabulary_.items()}\n",
    "    \n",
    "    # Train model\n",
    "    sepsis_model = XGBClassifier(random_state=0)\n",
    "    sepsis_model.fit(X_train_vect, Y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    sepsis_train_preds = sepsis_model.predict_proba(X_train_vect)[:,1]\n",
    "    sepsis_test_preds = sepsis_model.predict_proba(X_test_vect)[:,1]\n",
    "    \n",
    "    # Gathering AUCs and Brier scores\n",
    "    auc_train_sepsis.append(roc_auc_score(Y_train, sepsis_train_preds))\n",
    "    auc_test_sepsis.append(roc_auc_score(Y_test, sepsis_test_preds))\n",
    "    brier_train_sepsis.append(brier_score_loss(Y_train, sepsis_train_preds))\n",
    "    brier_test_sepsis.append(brier_score_loss(Y_test, sepsis_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(12,12))\n",
    "\n",
    "ax[0,0].boxplot(\n",
    "    [auc_train_pna, auc_train_chf, auc_train_aspiration, auc_train_sepsis],\n",
    "    labels=['PNA','CHF','Aspiration', 'Sepsis']\n",
    "    )\n",
    "ax[0,0].set(ylim=[0.5, 1.05])\n",
    "ax[0,0].set_ylabel(\"AUROC\", size=18)\n",
    "ax[0,0].tick_params(axis='x', labelsize=18)\n",
    "ax[0,0].tick_params(axis='y', labelsize=18)\n",
    "ax[0,0].set_title(\"Train AUC\")\n",
    "\n",
    "ax[0,1].boxplot(\n",
    "    [auc_test_pna, auc_test_chf, auc_test_aspiration, auc_test_sepsis],\n",
    "    labels=['PNA','CHF','Aspiration', 'Sepsis']\n",
    "    )\n",
    "ax[0,1].set_title(\"Test AUC\")\n",
    "ax[0,1].tick_params(axis='x', labelsize=18)\n",
    "ax[0,1].tick_params(axis='y', labelsize=18)\n",
    "ax[0,1].set(ylim=[0.5, 1.05])\n",
    "\n",
    "ax[1,0].boxplot(\n",
    "    [brier_train_pna, brier_train_chf, brier_train_aspiration, brier_train_sepsis],\n",
    "    labels=['PNA','CHF','Aspiration', 'Sepsis']\n",
    "    )\n",
    "ax[1,0].set_title(\"Train Brier\")\n",
    "ax[1,0].set_ylabel(\"Brier score\", size=18)\n",
    "ax[1,0].tick_params(axis='x', labelsize=18)\n",
    "ax[1,0].tick_params(axis='y', labelsize=18)\n",
    "ax[1,0].set(ylim=[0.00, 0.35])\n",
    "\n",
    "ax[1,1].boxplot(\n",
    "    [brier_test_pna, brier_test_chf, brier_test_aspiration, brier_test_sepsis],\n",
    "    labels=['PNA','CHF','Aspiration', 'Sepsis']\n",
    "    )\n",
    "ax[1,1].set_title(\"Test Brier\")\n",
    "ax[1,1].tick_params(axis='x', labelsize=18)\n",
    "ax[1,1].tick_params(axis='y', labelsize=18)\n",
    "ax[1,1].set(ylim=[0.00, 0.35])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48430448",
   "metadata": {},
   "source": [
    "### Now, hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0986fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "XG_param_grid = {\n",
    "    'base_score': hp.uniform('base_score', 0.0, 1.0),\n",
    "    'n_estimators': scope.int(hp.quniform(\"n_estimators\", 10, 10000, 10)),\n",
    "    'max_depth': scope.int(hp.quniform(\"max_depth\", 10, 10000, 10)),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.0, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0.0, 10.0),\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 0.0, 100.0),\n",
    "    'max_delta_step': hp.uniform(\"max_delta_step\", 0.0, 100.0),\n",
    "    'subsample': hp.uniform('subsample', 0.001, 1.0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ed63a",
   "metadata": {},
   "source": [
    "#### Pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3980d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization\n",
    "def objective(XG_param_grid):\n",
    "    cv = KFold()\n",
    "    logloss = Parallel(n_jobs=5)(delayed(custom_cv_func)(\n",
    "        XG_param_grid,\n",
    "        pneumonia_notes,\n",
    "        pneumonia_enctrs,\n",
    "        \"seg_pneumonia\",\n",
    "        \"pneumonia_sw\",\n",
    "        train_index,\n",
    "        test_index\n",
    "        ) for train_index, test_index in cv.split(pneumonia_enctrs))\n",
    "    \n",
    "    mean_logloss = np.mean(logloss)\n",
    "    var_logloss = np.var(logloss, ddof=1)\n",
    "\n",
    "    return {'loss': mean_logloss, 'loss_variance': var_logloss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bffe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_evals = 20*(# of ordinal hyperparams) + 15*(# categorical choices) = 160\n",
    "# stopping criteria, if needed: no improvement within window = 0.25*max_eval\n",
    "if __name__ == \"__main__\":\n",
    "    best_XG = fmin(\n",
    "        fn=objective,\n",
    "        space=XG_param_grid,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=160,\n",
    "        trials=Trials(),\n",
    "        early_stop_fn=no_progress_loss(40))\n",
    "    \n",
    "best_XG['n_estimators'] = int(best_XG['n_estimators'])\n",
    "best_XG['max_depth'] = int(best_XG['max_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Default AUROC for XG: {np.mean(auc_test_pna):.3f}\")\n",
    "print(f\"Default Brier for XG: {np.mean(brier_test_pna):.3f}\\n\")\n",
    "\n",
    "cv = KFold()\n",
    "auc = Parallel(n_jobs=5)(delayed(custom_cv_func)(\n",
    "    best_XG,\n",
    "    pneumonia_notes,\n",
    "    pneumonia_enctrs,\n",
    "    \"seg_pneumonia\",\n",
    "    \"pneumonia_sw\",\n",
    "    train_index,\n",
    "    test_index,\n",
    "    score='auc'\n",
    "    ) for train_index, test_index in cv.split(pneumonia_enctrs))\n",
    "\n",
    "brier = Parallel(n_jobs=5)(delayed(custom_cv_func)(\n",
    "    best_XG,\n",
    "    pneumonia_notes,\n",
    "    pneumonia_enctrs,\n",
    "    \"seg_pneumonia\",\n",
    "    \"pneumonia_sw\",\n",
    "    train_index,\n",
    "    test_index,\n",
    "    score='brier'\n",
    "    ) for train_index, test_index in cv.split(pneumonia_enctrs))\n",
    "\n",
    "print(f\"Tuned AUROC for XG: {np.mean(auc):.3f}\")\n",
    "print(f\"Tuned Brier for XG: {np.mean(brier):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12312e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pna_hyperparam = {\n",
    "  'base_score': str(best_XG['base_score']),\n",
    "  'n_estimators': str(int(best_XG['n_estimators'])),\n",
    "  'max_depth': str(int(best_XG['max_depth'])),\n",
    "  'learning_rate': str(best_XG['learning_rate']),\n",
    "  'gamma': str(best_XG['gamma']),\n",
    "  'min_child_weight': str(best_XG['min_child_weight']),\n",
    "  'max_delta_step': str(best_XG['max_delta_step']),\n",
    "  'subsample': str(best_XG['subsample'])\n",
    "  }\n",
    "\n",
    "pna_hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d521e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"hyperparameters/pna_XG_hyperparams.json\", 'w') as file_json:\n",
    "#     json.dump(pna_hyperparam, file_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5182b6fb",
   "metadata": {},
   "source": [
    "#### Congestive Heart Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa34786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(XG_param_grid):\n",
    "    cv = KFold()\n",
    "    logloss = Parallel(n_jobs=5)(delayed(custom_cv_func)(\n",
    "        XG_param_grid,\n",
    "        chf_notes,\n",
    "        chf_enctrs,\n",
    "        \"seg_chf\",\n",
    "        \"chf_sw\",\n",
    "        train_index,\n",
    "        test_index\n",
    "        ) for train_index, test_index in cv.split(chf_enctrs))\n",
    "    \n",
    "    mean_logloss = np.mean(logloss)\n",
    "    var_logloss = np.var(logloss, ddof=1)\n",
    "\n",
    "    return {'loss': mean_logloss, 'loss_variance': var_logloss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacedda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_evals = 20*(# of ordinal hyperparams) + 15*(# categorical choices) = 160\n",
    "# stopping criteria, if needed: no improvement within window = 0.25*max_eval\n",
    "if __name__ == \"__main__\":\n",
    "    best_XG = fmin(\n",
    "        fn=objective,\n",
    "        space=XG_param_grid,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=160,\n",
    "        trials=Trials(),\n",
    "        early_stop_fn=no_progress_loss(40)\n",
    "        )\n",
    "    \n",
    "best_XG['n_estimators'] = int(best_XG['n_estimators'])\n",
    "best_XG['max_depth'] = int(best_XG['max_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Default AUROC for XG: {np.mean(auc_test_chf):.3f}\")\n",
    "print(f\"Default Brier for XG: {np.mean(brier_test_chf):.3f}\\n\")\n",
    "\n",
    "cv = KFold()\n",
    "auc = Parallel(n_jobs=5)(delayed(custom_cv_func)(\n",
    "    best_XG,\n",
    "    chf_notes,\n",
    "    chf_enctrs,\n",
    "    \"seg_chf\",\n",
    "    \"chf_sw\",\n",
    "    train_index,\n",
    "    test_index,\n",
    "    score='auc'\n",
    "    ) for train_index, test_index in cv.split(chf_enctrs))\n",
    "\n",
    "brier = Parallel(n_jobs=5)(delayed(custom_cv_func)(\n",
    "    best_XG,\n",
    "    chf_notes,\n",
    "    chf_enctrs,\n",
    "    \"seg_chf\",\n",
    "    \"chf_sw\",\n",
    "    train_index,\n",
    "    test_index,\n",
    "    score='brier'\n",
    "    ) for train_index, test_index in cv.split(chf_enctrs))\n",
    "\n",
    "print(f\"Tuned AUROC for XG: {np.mean(auc):.3f}\")\n",
    "print(f\"Tuned Brier for XG: {np.mean(brier):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chf_hyperparam = {\n",
    "    'base_score': str(best_XG['base_score']),\n",
    "    'n_estimators': str(int(best_XG['n_estimators'])),\n",
    "    'max_depth': str(int(best_XG['max_depth'])),\n",
    "    'learning_rate': str(best_XG['learning_rate']),\n",
    "    'gamma': str(best_XG['gamma']),\n",
    "    'min_child_weight': str(best_XG['min_child_weight']),\n",
    "    'max_delta_step': str(best_XG['max_delta_step']),\n",
    "    'subsample': str(best_XG['subsample'])\n",
    "    }\n",
    "\n",
    "chf_hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e4606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"hyperparameters/chf_XG_hyperparams.json\", 'w') as file_json:\n",
    "#     json.dump(chf_hyperparam, file_json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f89e7987",
   "metadata": {},
   "source": [
    "#### Aspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5523980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(XG_param_grid):\n",
    "    cv = KFold()\n",
    "    logloss = Parallel(n_jobs=5)(delayed(custom_cv_func)(\n",
    "        XG_param_grid,\n",
    "        aspiration_notes,\n",
    "        aspiration_enctrs,\n",
    "        \"seg_aspiration\",\n",
    "        \"aspiration_sw\",\n",
    "        train_index,\n",
    "        test_index\n",
    "        ) for train_index, test_index in cv.split(aspiration_enctrs))\n",
    "    \n",
    "    mean_logloss = np.mean(logloss)\n",
    "    var_logloss = np.var(logloss, ddof=1)\n",
    "\n",
    "    return {'loss': mean_logloss, 'loss_variance': var_logloss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_evals = 20*(# of ordinal hyperparams) + 15*(# categorical choices) = 160\n",
    "# stopping criteria, if needed: no improvement within window = 0.25*max_eval\n",
    "if __name__ == \"__main__\":\n",
    "    best_XG = fmin(\n",
    "        fn=objective,\n",
    "        space=XG_param_grid,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=160,\n",
    "        trials=Trials(),\n",
    "        early_stop_fn=no_progress_loss(40)\n",
    "        )\n",
    "    \n",
    "best_XG['n_estimators'] = int(best_XG['n_estimators'])\n",
    "best_XG['max_depth'] = int(best_XG['max_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Default AUROC for XG: {np.mean(auc_test_aspiration):.3f}\")\n",
    "print(f\"Default Brier for XG: {np.mean(brier_test_aspiration):.3f}\\n\")\n",
    "\n",
    "cv = KFold()\n",
    "auc = Parallel(n_jobs=5)(delayed(custom_cv_func)(\n",
    "    best_XG,\n",
    "    aspiration_notes,\n",
    "    aspiration_enctrs,\n",
    "    \"seg_aspiration\",\n",
    "    \"aspiration_sw\",\n",
    "    train_index,\n",
    "    test_index,\n",
    "    score='auc'\n",
    "    ) for train_index, test_index in cv.split(aspiration_enctrs))\n",
    "\n",
    "brier = Parallel(n_jobs=5)(delayed(custom_cv_func)(\n",
    "    best_XG,\n",
    "    aspiration_notes,\n",
    "    aspiration_enctrs,\n",
    "    \"seg_aspiration\",\n",
    "    \"aspiration_sw\",\n",
    "    train_index,\n",
    "    test_index,\n",
    "    score='brier'\n",
    "    ) for train_index, test_index in cv.split(aspiration_enctrs))\n",
    "\n",
    "print(f\"Tuned AUROC for XG: {np.mean(auc):.3f}\")\n",
    "print(f\"Tuned Brier for XG: {np.mean(brier):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db68a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspiration_hyperparam = {\n",
    "    'base_score': str(best_XG['base_score']),\n",
    "    'n_estimators': str(int(best_XG['n_estimators'])),\n",
    "    'max_depth': str(int(best_XG['max_depth'])),\n",
    "    'learning_rate': str(best_XG['learning_rate']),\n",
    "    'gamma': str(best_XG['gamma']),\n",
    "    'min_child_weight': str(best_XG['min_child_weight']),\n",
    "    'max_delta_step': str(best_XG['max_delta_step']),\n",
    "    'subsample': str(best_XG['subsample'])\n",
    "    }\n",
    "\n",
    "aspiration_hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"hyperparameters/aspiration_XG_hyperparams.json\", 'w') as file_json:\n",
    "#     json.dump(aspiration_hyperparam, file_json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a13319bb",
   "metadata": {},
   "source": [
    "#### Sepsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4009427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(XG_param_grid):\n",
    "    cv = KFold()\n",
    "    logloss = Parallel(n_jobs=5)(delayed(custom_cv_func)(\n",
    "        XG_param_grid,\n",
    "        sepsis_notes,\n",
    "        sepsis_enctrs,\n",
    "        \"seg_sepsis\",\n",
    "        \"sepsis_sw\",\n",
    "        train_index,\n",
    "        test_index\n",
    "        ) for train_index, test_index in cv.split(sepsis_enctrs))\n",
    "    \n",
    "    mean_logloss = np.mean(logloss)\n",
    "    var_logloss = np.var(logloss, ddof=1)\n",
    "\n",
    "    return {'loss': mean_logloss, 'loss_variance': var_logloss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bbac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_evals = 20*(# of ordinal hyperparams) + 15*(# categorical choices) = 140\n",
    "# stopping criteria, if needed: no improvement within window = 0.25*max_eval\n",
    "if __name__ == \"__main__\":\n",
    "    best_XG = fmin(\n",
    "        fn=objective,\n",
    "        space=XG_param_grid,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=160,\n",
    "        trials=Trials(),\n",
    "        early_stop_fn=no_progress_loss(40))\n",
    "    \n",
    "best_XG['n_estimators'] = int(best_XG['n_estimators'])\n",
    "best_XG['max_depth'] = int(best_XG['max_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Default AUROC for XG: {np.mean(auc_test_sepsis):.3f}\")\n",
    "print(f\"Default Brier for XG: {np.mean(brier_test_sepsis):.3f}\\n\")\n",
    "\n",
    "cv = KFold()\n",
    "auc = Parallel(n_jobs=5)(delayed(custom_cv_func)(\n",
    "    best_XG,\n",
    "    sepsis_notes,\n",
    "    sepsis_enctrs,\n",
    "    \"seg_sepsis\",\n",
    "    \"sepsis_sw\",\n",
    "    train_index,\n",
    "    test_index,\n",
    "    score='auc'\n",
    "    ) for train_index, test_index in cv.split(sepsis_enctrs))\n",
    "\n",
    "brier = Parallel(n_jobs=5)(delayed(custom_cv_func)(\n",
    "    best_XG,\n",
    "    sepsis_notes,\n",
    "    sepsis_enctrs,\n",
    "    \"seg_sepsis\",\n",
    "    \"sepsis_sw\",\n",
    "    train_index,\n",
    "    test_index,\n",
    "    score='brier'\n",
    "    ) for train_index, test_index in cv.split(sepsis_enctrs))\n",
    "\n",
    "print(f\"Tuned AUROC for XG: {np.mean(auc):.3f}\")\n",
    "print(f\"Tuned Brier for XG: {np.mean(brier):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fca0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sepsis_hyperparam = {\n",
    "    'base_score': str(best_XG['base_score']),\n",
    "    'n_estimators': str(int(best_XG['n_estimators'])),\n",
    "    'max_depth': str(int(best_XG['max_depth'])),\n",
    "    'learning_rate': str(best_XG['learning_rate']),\n",
    "    'gamma': str(best_XG['gamma']),\n",
    "    'min_child_weight': str(best_XG['min_child_weight']),\n",
    "    'max_delta_step': str(best_XG['max_delta_step']),\n",
    "    'subsample': str(best_XG['subsample'])\n",
    "    }\n",
    "\n",
    "sepsis_hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcac184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"hyperparameters/sepsis_XG_hyperparams.json\", 'w') as file_json:\n",
    "#     json.dump(sepsis_hyperparam, file_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
