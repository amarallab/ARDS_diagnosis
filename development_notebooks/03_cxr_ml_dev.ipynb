{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is concerned with running various ML models on chest imaging reports, and various aspects of its implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm   # For keeping track of loops\n",
    "from joblib import Parallel, delayed\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom imports\n",
    "from custom_functions import *\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import src.plots as plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plotting params\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.style.reload_library()\n",
    "rcparams = plots.stdrcparams1()\n",
    "mpl.rcParams.update(rcparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models/algorithms/classifiers\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Evaluation of models\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# Text vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data: Hospital A (2013), Hospital A (2016), Hospital B (2017-18), and MIMIC III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data locations\n",
    "basedir = Path(\"../..\")\n",
    "training_path = basedir / \"Analysis_Data\" / \"train_ML\"\n",
    "train_data1a = training_path / 'hospital_a_2013_bi_data_processed_minus_history_plus_conclusion-03-2021.csv'\n",
    "train_data1b = training_path / 'hospital_a_2013_cxr_annotated.csv'\n",
    "\n",
    "train_data2 = training_path / 'hospital_a_2016_bi_data_processed_minus_history_plus_conclusion.csv'\n",
    "\n",
    "test_data3a = training_path / 'hospital_b_2017_cxr_annotated.csv'\n",
    "test_data3b = training_path / 'hospital_b_2017_cxr_annotated_2023_jesse_curt_merged.csv'\n",
    "test_data3c = training_path / 'hospital_b_2017_cxr_annotated_2023_three_annotators_merged.csv'\n",
    "test_data3_whole = basedir / \"Analysis_Data\" / \"hospital_b_2017\" / 'cxr.csv'\n",
    "\n",
    "whole_cxr_training = training_path / \"cxr_whole_training_dataset.csv\"\n",
    "\n",
    "mimic3_path = basedir / \"Analysis_Data\" / \"MIMIC_III\" / \"labeled_subset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figures\n",
    "figure_path = basedir / \"Figures\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospital A (2013) read in and minor processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust which to read in desired Hospital A (2013) CXR reports\n",
    "# 'a' is for originally processed file\n",
    "# 'b' is for file processed by pipeline\n",
    "which = 'b'\n",
    "cols = {\n",
    "        'a': ['segmented_report', 'score'],\n",
    "        'b': ['seg_cxr_text', 'cxr_score']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if which == 'a':\n",
    "    # open Hospital A (2013) processed files - segmented_reports\n",
    "    segmented = pd.read_csv(train_data1a)\n",
    "    segmented = segmented.drop(columns='Unnamed: 0').drop_duplicates()\n",
    "    \n",
    "elif which == 'b':\n",
    "    # open Hospital A (2013) processed files - segmented_reports\n",
    "    segmented = pd.read_csv(train_data1b)\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"Type either 'a' or 'b', lower case.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing remaining punctuation marks.\n",
    "segmented[cols[which][0]] = segmented[cols[which][0]].str.replace(r\"'\", r\"\", regex=True)\n",
    "segmented[cols[which][0]] = segmented[cols[which][0]].str.replace(r\"\\[\", r\"\", regex=True)\n",
    "segmented[cols[which][0]] = segmented[cols[which][0]].str.replace(r\"]\", r\"\", regex=True)\n",
    "segmented[cols[which][0]] = segmented[cols[which][0]].str.replace(r\",\", r\"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospital A (2016) read in and minor processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_cohort2 = pd.read_csv(train_data2)\n",
    "\n",
    "# Replace some remaining punctuation marks\n",
    "segmented_cohort2[\"segmented_report\"] = segmented_cohort2[\"segmented_report\"].str.replace(r\"'\", r\"\", regex=True)\n",
    "segmented_cohort2[\"segmented_report\"] = segmented_cohort2[\"segmented_report\"].str.replace(r\"\\[\", r\"\", regex=True)\n",
    "segmented_cohort2[\"segmented_report\"] = segmented_cohort2[\"segmented_report\"].str.replace(r\"\\]\", r\"\", regex=True)\n",
    "segmented_cohort2[\"segmented_report\"] = segmented_cohort2[\"segmented_report\"].str.replace(r\",\", r\"\", regex=True)\n",
    "segmented_cohort2 = segmented_cohort2.drop(columns='Unnamed: 0').drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospital B (2017-2018) read in and minor processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_cohort3_original = pd.read_csv(test_data3a)\n",
    "\n",
    "# Removing remaining punctuation marks\n",
    "segmented_cohort3_original['seg_cxr_text'] = segmented_cohort3_original['seg_cxr_text'].str.replace(r\"'\", r\"\", regex=True)\n",
    "segmented_cohort3_original['seg_cxr_text'] = segmented_cohort3_original['seg_cxr_text'].str.replace(r\"\\[\", r\"\", regex=True)\n",
    "segmented_cohort3_original['seg_cxr_text'] = segmented_cohort3_original['seg_cxr_text'].str.replace(r\"\\]\", r\"\", regex=True)\n",
    "segmented_cohort3_original['seg_cxr_text'] = segmented_cohort3_original['seg_cxr_text'].str.replace(r\",\", r\"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CXR whole training dataset read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(whole_cxr_training)\n",
    "\n",
    "# Removing remaining punctuation marks\n",
    "training_data['seg_cxr_text'] = training_data['seg_cxr_text'].str.replace(r\"'\", r\"\", regex=True)\n",
    "training_data['seg_cxr_text'] = training_data['seg_cxr_text'].str.replace(r\"\\[\", r\"\", regex=True)\n",
    "training_data['seg_cxr_text'] = training_data['seg_cxr_text'].str.replace(r\"\\]\", r\"\", regex=True)\n",
    "training_data['seg_cxr_text'] = training_data['seg_cxr_text'].str.replace(r\",\", r\"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating bootstrapped AUCs, calibration, and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_boot = 100   # Make smaller if wanting the notebook to run faster.\n",
    "n_boot1 = 10\n",
    "\n",
    "XG_test_auc = []\n",
    "XG_tprs = []\n",
    "XG_fops = []\n",
    "XG_preds = []\n",
    "XG_rsquared = []\n",
    "XG_dws = []\n",
    "XG_importances = []\n",
    "XG_shap = []\n",
    "XG_test_set = []\n",
    "\n",
    "LR_test_auc = []\n",
    "LR_tprs = []\n",
    "LR_fops = []\n",
    "LR_preds = []\n",
    "LR_rsquared = []\n",
    "LR_dws = []\n",
    "LR_importances = []\n",
    "LR_shap = []\n",
    "LR_test_set = []\n",
    "\n",
    "RF_test_auc = []\n",
    "RF_tprs = []\n",
    "RF_fops = []\n",
    "RF_preds = []\n",
    "RF_rsquared = []\n",
    "RF_dws = []\n",
    "RF_importances = []\n",
    "RF_shap = []\n",
    "RF_test_set = []\n",
    "\n",
    "DT_test_auc = []\n",
    "DT_tprs = []\n",
    "DT_fops = []\n",
    "DT_preds = []\n",
    "DT_rsquared = []\n",
    "DT_dws = []\n",
    "DT_importances = []\n",
    "DT_shap = []\n",
    "DT_test_set = []\n",
    "\n",
    "mean_fpr = np.arange(0, 1.01, 0.01)\n",
    "mean_mpv = np.arange(0, 1.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of graphs generated by this notebook depend on this loop, including the calibration curve\n",
    "# (which is part of the panel where the interrater agreement graph is)\n",
    "for i in tqdm(range(n_boot1)):\n",
    "\n",
    "    # This line resamples the data, WITH replacement\n",
    "    boot_segmented = training_data.sample(n=len(training_data), replace=True, axis=0)\n",
    "    \n",
    "    encounters = boot_segmented['encounter_id'].unique()\n",
    "    cv = KFold()\n",
    "    \n",
    "    # # Accounting for potential label imbalance\n",
    "    # count_0 = pd.Series(y_train).value_counts()[0]\n",
    "    # count_1 = pd.Series(y_train).value_counts()[1]\n",
    "    # weight_0 = count_0 / len(y_train)\n",
    "    # weight_1 = count_1 / len(y_train)\n",
    "    # print(f\" %No: {weight_0*100:.2f}%, %Yes: {weight_1*100:.2f}%\")\n",
    "    \n",
    "    \n",
    "    # Training each model\n",
    "    print(\"Working on XGBoost\")\n",
    "    if __name__ == '__main__':\n",
    "        output1, output2, output3, output4, output5 = Parallel(n_jobs=15)(delayed(nested_cv)(\n",
    "            boot_segmented,\n",
    "            cols,\n",
    "            which,\n",
    "            encounters,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            mean_fpr,\n",
    "            mean_mpv,\n",
    "            model=\"XGBoost\",\n",
    "            score='log_loss'\n",
    "            ) for train_index, test_index in cv.split(encounters))\n",
    "    \n",
    "    XG_test_auc.append(np.mean([output1[0], output2[0], output3[0], output4[0], output5[0]]))\n",
    "    XG_preds.extend(output1[1])\n",
    "    XG_preds.extend(output2[1])\n",
    "    XG_preds.extend(output3[1])\n",
    "    XG_preds.extend(output4[1])\n",
    "    XG_preds.extend(output5[1])\n",
    "    XG_tprs.append(np.mean([output1[2], output2[2], output3[2], output4[2], output5[2]], axis=0))\n",
    "    XG_fops.append(np.mean([output1[3], output2[3], output3[3], output4[3], output5[3]], axis=0))\n",
    "    XG_rsquared.append(np.mean([output1[4], output2[4], output3[4], output4[4], output5[4]], axis=0))\n",
    "    XG_dws.append(np.mean([output1[5], output2[5], output3[5], output4[5], output5[5]], axis=0))\n",
    "    XG_importances.extend(pd.DataFrame(\n",
    "        output1[6]+output2[6]+output3[6]+output4[6]+output5[6]).groupby('feature').mean().reset_index().to_dict(orient='records'))\n",
    "    \n",
    "\n",
    "    print(\"Working on Logistic Regression\")\n",
    "    if __name__ == '__main__':\n",
    "        output1, output2, output3, output4, output5 = Parallel(n_jobs=15)(delayed(nested_cv)(\n",
    "            boot_segmented,\n",
    "            cols,\n",
    "            which,\n",
    "            encounters,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            mean_fpr,\n",
    "            mean_mpv,\n",
    "            model=\"LogisticRegression\",\n",
    "            score='log_loss'\n",
    "            ) for train_index, test_index in cv.split(encounters))\n",
    "    \n",
    "    LR_test_auc.append(np.mean([output1[0], output2[0], output3[0], output4[0], output5[0]]))\n",
    "    LR_preds.extend(output1[1])\n",
    "    LR_preds.extend(output2[1])\n",
    "    LR_preds.extend(output3[1])\n",
    "    LR_preds.extend(output4[1])\n",
    "    LR_preds.extend(output5[1])\n",
    "    LR_tprs.append(np.mean([output1[2], output2[2], output3[2], output4[2], output5[2]], axis=0))\n",
    "    LR_fops.append(np.mean([output1[3], output2[3], output3[3], output4[3], output5[3]], axis=0))\n",
    "    LR_rsquared.append(np.mean([output1[4], output2[4], output3[4], output4[4], output5[4]], axis=0))\n",
    "    LR_dws.append(np.mean([output1[5], output2[5], output3[5], output4[5], output5[5]], axis=0))\n",
    "    LR_importances.extend(pd.DataFrame(\n",
    "        output1[6]+output2[6]+output3[6]+output4[6]+output5[6]).groupby('feature').mean().reset_index().to_dict(orient='records'))\n",
    "\n",
    "\n",
    "    print(\"Working on Random Forest\")\n",
    "    if __name__ == '__main__':\n",
    "        output1, output2, output3, output4, output5 = Parallel(n_jobs=15)(delayed(nested_cv)(\n",
    "            boot_segmented,\n",
    "            cols,\n",
    "            which,\n",
    "            encounters,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            mean_fpr,\n",
    "            mean_mpv,\n",
    "            model=\"RandomForest\",\n",
    "            score='log_loss'\n",
    "            ) for train_index, test_index in cv.split(encounters))\n",
    "    \n",
    "    RF_test_auc.append(np.mean([output1[0], output2[0], output3[0], output4[0], output5[0]]))\n",
    "    RF_preds.extend(output1[1])\n",
    "    RF_preds.extend(output2[1])\n",
    "    RF_preds.extend(output3[1])\n",
    "    RF_preds.extend(output4[1])\n",
    "    RF_preds.extend(output5[1])\n",
    "    RF_tprs.append(np.mean([output1[2], output2[2], output3[2], output4[2], output5[2]], axis=0))\n",
    "    RF_fops.append(np.mean([output1[3], output2[3], output3[3], output4[3], output5[3]], axis=0))\n",
    "    RF_rsquared.append(np.mean([output1[4], output2[4], output3[4], output4[4], output5[4]], axis=0))\n",
    "    RF_dws.append(np.mean([output1[5], output2[5], output3[5], output4[5], output5[5]], axis=0))\n",
    "    RF_importances.extend(pd.DataFrame(\n",
    "        output1[6]+output2[6]+output3[6]+output4[6]+output5[6]).groupby('feature').mean().reset_index().to_dict(orient='records'))\n",
    "\n",
    "\n",
    "    print(\"Working on Decision Tree\")\n",
    "    if __name__ == '__main__':\n",
    "        output1, output2, output3, output4, output5 = Parallel(n_jobs=15)(delayed(nested_cv)(\n",
    "            boot_segmented,\n",
    "            cols,\n",
    "            which,\n",
    "            encounters,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            mean_fpr,\n",
    "            mean_mpv,\n",
    "            model=\"DecisionTree\",\n",
    "            score='log_loss'\n",
    "            ) for train_index, test_index in cv.split(encounters))\n",
    "        \n",
    "    DT_test_auc.append(np.mean([output1[0], output2[0], output3[0], output4[0], output5[0]]))\n",
    "    DT_preds.extend(output1[1])\n",
    "    DT_preds.extend(output2[1])\n",
    "    DT_preds.extend(output3[1])\n",
    "    DT_preds.extend(output4[1])\n",
    "    DT_preds.extend(output5[1])\n",
    "    DT_tprs.append(np.mean([output1[2], output2[2], output3[2], output4[2], output5[2]], axis=0))\n",
    "    DT_fops.append(np.mean([output1[3], output2[3], output3[3], output4[3], output5[3]], axis=0))\n",
    "    DT_rsquared.append(np.mean([output1[4], output2[4], output3[4], output4[4], output5[4]], axis=0))\n",
    "    DT_dws.append(np.mean([output1[5], output2[5], output3[5], output4[5], output5[5]], axis=0))\n",
    "    DT_importances.extend(pd.DataFrame( \n",
    "        output1[6]+output2[6]+output3[6]+output4[6]+output5[6]).groupby('feature').mean().reset_index().to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifying data collected in the loop, for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "mean_DT_tpr = np.mean(DT_tprs, axis=0)\n",
    "mean_DT_tpr[-1] = 0.0\n",
    "DT_tpr_CI95 = [\n",
    "        np.percentile(DT_tprs, 2.5, axis=0),\n",
    "        np.percentile(DT_tprs, 97.5, axis=0)\n",
    "        ]\n",
    "\n",
    "mean_DT_auc = np.mean(DT_test_auc)\n",
    "DT_auc_CI95 = [\n",
    "        np.percentile(DT_test_auc, 2.5),\n",
    "        np.percentile(DT_test_auc, 97.5)\n",
    "        ]\n",
    "\n",
    "DT_importances_df = pd.DataFrame(DT_importances).groupby('feature').agg(\n",
    "    mean_imp=('importance', np.mean),\n",
    "    low_CI=('importance', lambda x: x.mean() - x.quantile(0.025)),\n",
    "    high_CI=('importance', lambda x: x.quantile(0.975) - x.mean())).reset_index().sort_values(\n",
    "            by='mean_imp',\n",
    "            ascending=False\n",
    "            )\n",
    "\n",
    "mean_DT_fop = np.mean(DT_fops, axis=0)\n",
    "DT_fop_CI95 = [\n",
    "        np.percentile(DT_fops, 2.5, axis=0),\n",
    "        np.percentile(DT_fops, 97.5, axis=0)\n",
    "        ]\n",
    "mean_DT_rsquared = np.mean(DT_rsquared, axis=0)\n",
    "DT_rsquared_CI95 = [\n",
    "        np.percentile(DT_rsquared, 2.5, axis=0),\n",
    "        np.percentile(DT_rsquared, 97.5, axis=0)\n",
    "        ]\n",
    "mean_DT_dws = np.mean(DT_dws, axis=0)\n",
    "DT_dws_CI95 = [\n",
    "        np.percentile(DT_dws, 2.5, axis=0),\n",
    "        np.percentile(DT_dws, 97.5, axis=0)\n",
    "        ]\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "mean_LR_tpr = np.mean(LR_tprs, axis=0)\n",
    "mean_LR_tpr[-1] = 0.0\n",
    "LR_tpr_CI95 = [\n",
    "        np.percentile(LR_tprs, 2.5, axis=0),\n",
    "        np.percentile(LR_tprs, 97.5, axis=0)\n",
    "        ]\n",
    "\n",
    "mean_LR_auc = np.mean(LR_test_auc)\n",
    "LR_auc_CI95 = [\n",
    "        np.percentile(LR_test_auc, 2.5),\n",
    "        np.percentile(LR_test_auc, 97.5)\n",
    "        ]\n",
    "\n",
    "LR_importances_df = pd.DataFrame(LR_importances).groupby('feature').agg(\n",
    "    mean_imp=('importance', np.mean),\n",
    "    low_CI=('importance', lambda x: x.mean() - x.quantile(0.025)),\n",
    "    high_CI=('importance', lambda x: x.quantile(0.975) - x.mean())).reset_index().sort_values(\n",
    "            by='mean_imp',\n",
    "            ascending=False,\n",
    "            key=np.absolute\n",
    "            )\n",
    "\n",
    "mean_LR_fop = np.mean(LR_fops, axis=0)\n",
    "LR_fop_CI95 = [\n",
    "        np.percentile(LR_fops, 2.5, axis=0),\n",
    "        np.percentile(LR_fops, 97.5, axis=0)\n",
    "        ]\n",
    "mean_LR_rsquared = np.mean(LR_rsquared, axis=0)\n",
    "LR_rsquared_CI95 = [\n",
    "        np.percentile(LR_rsquared, 2.5, axis=0),\n",
    "        np.percentile(LR_rsquared, 97.5, axis=0)\n",
    "        ]\n",
    "mean_LR_dws = np.mean(LR_dws, axis=0)\n",
    "LR_dws_CI95 = [\n",
    "        np.percentile(LR_dws, 2.5, axis=0),\n",
    "        np.percentile(LR_dws, 97.5, axis=0)\n",
    "        ]\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "mean_RF_tpr = np.mean(RF_tprs, axis=0)\n",
    "mean_RF_tpr[-1] = 0.0\n",
    "RF_tpr_CI95 = [\n",
    "        np.percentile(RF_tprs, 2.5, axis=0),\n",
    "        np.percentile(RF_tprs, 97.5, axis=0)\n",
    "        ]\n",
    "\n",
    "mean_RF_auc = np.mean(RF_test_auc)\n",
    "RF_auc_CI95 = [\n",
    "        np.percentile(RF_test_auc, 2.5),\n",
    "        np.percentile(RF_test_auc, 97.5)\n",
    "        ]\n",
    "\n",
    "RF_importances_df = pd.DataFrame(RF_importances).groupby('feature').agg(\n",
    "    mean_imp=('importance', np.mean),\n",
    "    low_CI=('importance', lambda x: x.mean() - x.quantile(0.025)),\n",
    "    high_CI=('importance', lambda x: x.quantile(0.975) - x.mean())).reset_index().sort_values(\n",
    "            by='mean_imp',\n",
    "            ascending=False\n",
    "            )\n",
    "\n",
    "mean_RF_fop = np.mean(RF_fops, axis=0)\n",
    "RF_fop_CI95 = [\n",
    "        np.percentile(RF_fops, 2.5, axis=0),\n",
    "        np.percentile(RF_fops, 97.5, axis=0)\n",
    "        ]\n",
    "mean_RF_rsquared = np.mean(RF_rsquared, axis=0)\n",
    "RF_rsquared_CI95 = [\n",
    "        np.percentile(RF_rsquared, 2.5, axis=0),\n",
    "        np.percentile(RF_rsquared, 97.5, axis=0)\n",
    "        ]\n",
    "mean_RF_dws = np.mean(RF_dws, axis=0)\n",
    "RF_dws_CI95 = [\n",
    "        np.percentile(RF_dws, 2.5, axis=0),\n",
    "        np.percentile(RF_dws, 97.5, axis=0)\n",
    "        ]\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "mean_XG_tpr = np.mean(XG_tprs, axis=0)\n",
    "mean_XG_tpr[-1] = 0.0\n",
    "XG_tpr_CI95 = [\n",
    "        np.percentile(XG_tprs, 2.5, axis=0),\n",
    "        np.percentile(XG_tprs, 97.5, axis=0)\n",
    "        ]\n",
    "\n",
    "mean_XG_auc = np.mean(XG_test_auc)\n",
    "XG_auc_CI95 = [\n",
    "        np.percentile(XG_test_auc, 2.5),\n",
    "        np.percentile(XG_test_auc, 97.5)\n",
    "        ]\n",
    "\n",
    "XG_importances_df = pd.DataFrame(XG_importances).groupby('feature').agg(\n",
    "    mean_imp=('importance', np.mean),\n",
    "    low_CI=('importance', lambda x: x.mean() - x.quantile(0.025)),\n",
    "    high_CI=('importance', lambda x: x.quantile(0.975) - x.mean())).reset_index().sort_values(\n",
    "            by='mean_imp',\n",
    "            ascending=False\n",
    "            )\n",
    "\n",
    "mean_XG_fop = np.mean(XG_fops, axis=0)\n",
    "XG_fop_CI95 = [\n",
    "        np.percentile(XG_fops, 2.5, axis=0),\n",
    "        np.percentile(XG_fops, 97.5, axis=0)\n",
    "        ]\n",
    "mean_XG_rsquared = np.mean(XG_rsquared, axis=0)\n",
    "XG_rsquared_CI95 = [\n",
    "        np.percentile(XG_rsquared, 2.5, axis=0),\n",
    "        np.percentile(XG_rsquared, 97.5, axis=0)\n",
    "        ]\n",
    "mean_XG_dws = np.mean(XG_dws, axis=0)\n",
    "XG_dws_CI95 = [\n",
    "        np.percentile(XG_dws, 2.5, axis=0),\n",
    "        np.percentile(XG_dws, 97.5, axis=0)\n",
    "        ]\n",
    "\n",
    "# For AUC\n",
    "models = ['Decision\\nTree', 'Logistic\\nRegression', 'Random\\nForest', 'XGBoost']\n",
    "heights = [mean_DT_auc, mean_LR_auc, mean_RF_auc, mean_XG_auc]\n",
    "CI95 = [\n",
    "        [\n",
    "                heights[0] - DT_auc_CI95[0],\n",
    "                heights[1] - LR_auc_CI95[0],\n",
    "                heights[2] - RF_auc_CI95[0],\n",
    "                heights[3] - XG_auc_CI95[0]\n",
    "        ],\n",
    "        [\n",
    "                DT_auc_CI95[1] - heights[0],\n",
    "                LR_auc_CI95[1] - heights[1],\n",
    "                RF_auc_CI95[1] - heights[2],\n",
    "                XG_auc_CI95[1] - heights[3]\n",
    "        ]\n",
    "       ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting ROC curves with uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\", alpha=0.8)\n",
    "\n",
    "ax.plot(mean_fpr, mean_DT_tpr, color='#66c2a5', label=f\"{heights[0]:.2f} ({DT_auc_CI95[0]:.2f}-{DT_auc_CI95[1]:.2f})\")\n",
    "ax.fill_between(mean_fpr, DT_tpr_CI95[0], DT_tpr_CI95[1],\n",
    "                color='#66c2a5', alpha=0.2)\n",
    "\n",
    "ax.plot(mean_fpr, mean_LR_tpr, color='#fc8d62', label=f\"{heights[1]:.2f} ({LR_auc_CI95[0]:.2f}-{LR_auc_CI95[1]:.2f})\")\n",
    "ax.fill_between(mean_fpr, LR_tpr_CI95[0], LR_tpr_CI95[1],\n",
    "                color='#fc8d62', alpha=0.2)\n",
    "\n",
    "ax.plot(mean_fpr, mean_RF_tpr, color='#8da0cb', label=f\"{heights[2]:.2f} ({RF_auc_CI95[0]:.2f}-{RF_auc_CI95[1]:.2f})\")\n",
    "ax.fill_between(mean_fpr, RF_tpr_CI95[0], RF_tpr_CI95[1],\n",
    "                color='#8da0cb', alpha=0.2)\n",
    "\n",
    "ax.plot(mean_fpr, mean_XG_tpr, color='#e78ac3', label=f\"{heights[3]:.2f} ({XG_auc_CI95[0]:.2f}-{XG_auc_CI95[1]:.2f})\")\n",
    "ax.fill_between(mean_fpr, XG_tpr_CI95[0], XG_tpr_CI95[1],\n",
    "                color='#e78ac3', alpha=0.2)\n",
    "\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax.grid(linestyle=':')\n",
    "ax.legend(loc='best', frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figure_path / \"fig1_ROC_curves_all_models.pdf\", dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(figsize=(7,7))\n",
    "\n",
    "ax1.plot(mean_fpr, mean_DT_tpr, color='#66c2a5', label=f\"Decision tree performance:\\n{mean_DT_auc:.2f}\")\n",
    "ax1.fill_between(mean_fpr, DT_tpr_CI95[0], DT_tpr_CI95[1],\n",
    "                 color='#66c2a5', alpha=0.2)\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\", alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel(\"True Positive Rate\")\n",
    "ax1.set_xlabel(\"False Positive Rate\")\n",
    "# ax1.set_title(\"Decision Tree\")\n",
    "ax1.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax1.grid(linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figure_path / \"ROC_curves_DT.pdf\", dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots(figsize=(7,7))\n",
    "\n",
    "ax2.plot(mean_fpr, mean_LR_tpr, color='#fc8d62', label=f\"Logistic regression performance:\\n{mean_LR_auc:.2f}\")\n",
    "ax2.fill_between(mean_fpr, LR_tpr_CI95[0], LR_tpr_CI95[1],\n",
    "                 alpha=0.2, color='#fc8d62')\n",
    "ax2.plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\", alpha=0.8)\n",
    "\n",
    "ax2.set_ylabel(\"True Positive Rate\")\n",
    "ax2.set_xlabel(\"False Positive Rate\")\n",
    "# ax2.set_title(\"Logistic Regression\")\n",
    "ax2.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax2.grid(linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figure_path / \"ROC_curves_LR.pdf\", dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, ax3 = plt.subplots(figsize=(7,7))\n",
    "\n",
    "ax3.plot(mean_fpr, mean_RF_tpr, color='#8da0cb', label=f\"Random Forest performance:\\n{mean_RF_auc:.2f}\")\n",
    "ax3.fill_between(mean_fpr, RF_tpr_CI95[0], RF_tpr_CI95[1],\n",
    "                 color='#8da0cb', alpha=0.2)\n",
    "ax3.plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\", alpha=0.8)\n",
    "\n",
    "ax3.set_ylabel(\"True Positive Rate\")\n",
    "ax3.set_xlabel(\"False Positive Rate\")\n",
    "# ax3.set_title(\"Random Forest\")\n",
    "ax3.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax3.grid(linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figure_path / \"ROC_curves_RF.pdf\", dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4, ax4 = plt.subplots(figsize=(7,7))\n",
    "\n",
    "ax4.plot(mean_fpr, mean_XG_tpr, color='#e78ac3', label=f\"XGBoost performance:\\n{mean_XG_auc:.2f}\")\n",
    "ax4.fill_between(mean_fpr, XG_tpr_CI95[0], XG_tpr_CI95[1],\n",
    "                 color='#e78ac3', alpha=0.2)\n",
    "ax4.plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\", alpha=0.8)\n",
    "\n",
    "ax4.set_ylabel(\"True Positive Rate\")\n",
    "ax4.set_xlabel(\"False Positive Rate\")\n",
    "# ax4.set_title(\"XGBoost\")\n",
    "ax4.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax4.grid(linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figure_path / \"ROC_curves_XG.pdf\", dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5, ax5 = plt.subplots(figsize=plots.stdfigsize())\n",
    "ax5.bar(models, heights, color=['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3'],\n",
    "        yerr=CI95, capsize=5)\n",
    "\n",
    "ax5.set_ylabel('Performance (AUROC)')\n",
    "ax5.set_xlabel('')\n",
    "ax5.grid(linestyle=':', axis='y')\n",
    "ax5.set_ylim(0., .5)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figure_path / 'fig1_auc_different_models.pdf', dpi=1000)\n",
    "# plt.savefig(figure_path / 'fig1_auc_different_models.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Plotting feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_words = ['edema', 'pulmonary edema', 'bilateral', 'atelectasis', 'diffuse', 'interstitial', 'no', 'clear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR1 = LR_importances_df.nlargest(7,\n",
    "                                 columns=\"mean_imp\",\n",
    "                                 keep='all')\n",
    "\n",
    "LR2 = LR_importances_df.nsmallest(7,\n",
    "                                  columns=\"mean_imp\",\n",
    "                                  keep='all').sort_values(by='mean_imp', ascending=False)\n",
    "\n",
    "LR_imp = pd.concat([LR1, LR2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig6, ax6 = plt.subplots(figsize=plots.stdfigsize())\n",
    "\n",
    "DT_importances_df.nlargest(15,\n",
    "                           columns=\"mean_imp\",\n",
    "                           keep='all').sort_values(by='mean_imp').plot(x='feature',\n",
    "                                                                       y='mean_imp',\n",
    "                                                                       kind='barh',\n",
    "                                                                       color='#66c2a5',\n",
    "                                                                       ax=ax6,\n",
    "                                                                       capsize=4,\n",
    "                                                                       xerr=[list(DT_importances_df.nlargest(15,\n",
    "                                                                                                             columns=\"mean_imp\",\n",
    "                                                                                                             keep='all')['low_CI']),\n",
    "                                                                             list(DT_importances_df.nlargest(15,\n",
    "                                                                                                             columns=\"mean_imp\",\n",
    "                                                                                                             keep='all')['high_CI'])],\n",
    "                                                                       label='Decision Tree')\n",
    "\n",
    "for tick in ax6.yaxis.get_major_ticks():\n",
    "     if tick.label1.get_text() in imp_words:\n",
    "         tick.label1.set_fontweight('bold')\n",
    "     else:\n",
    "         pass\n",
    "ax6.set_xlabel('Importance')\n",
    "ax6.set_ylabel('')\n",
    "ax6.grid(linestyle=':', axis='x')\n",
    "ax6.legend(loc='best', frameon=False)\n",
    "# ax6.set_title(\"Decision Tree\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figure_path / 'fig1_importance_DT.pdf', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig7, ax7 = plt.subplots(figsize=plots.stdfigsize())\n",
    "\n",
    "LR_imp.plot(x='feature',\n",
    "            y='mean_imp',\n",
    "            kind='barh',\n",
    "            color='#fc8d62',\n",
    "            ax=ax7,\n",
    "            capsize=4,\n",
    "            xerr=[list(LR_imp.nlargest(15,\n",
    "                                       columns=\"mean_imp\",\n",
    "                                       keep='all')['low_CI']),\n",
    "                  list(LR_imp.nlargest(15,\n",
    "                                       columns=\"mean_imp\",\n",
    "                                       keep='all')['high_CI'])],\n",
    "            label=\"Logistic\\nRegression\")\n",
    "\n",
    "for tick in ax7.yaxis.get_major_ticks():\n",
    "     if tick.label1.get_text() in imp_words:\n",
    "         tick.label1.set_fontweight('bold')\n",
    "     else:\n",
    "         pass\n",
    "ax7.set_xlabel('Importance')\n",
    "ax7.set_ylabel('')\n",
    "ax7.grid(linestyle=':', axis='x')\n",
    "ax7.legend(loc='best', frameon=False)\n",
    "# ax7.set_title(\"Logistic Regression\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figure_path /'fig1_importance_LR.pdf', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig8, ax8 = plt.subplots(figsize=plots.stdfigsize())\n",
    "\n",
    "RF_importances_df.nlargest(15,\n",
    "                           columns=\"mean_imp\",\n",
    "                           keep='all').sort_values(by='mean_imp').plot(x='feature',\n",
    "                                                                       y='mean_imp',\n",
    "                                                                       kind='barh',\n",
    "                                                                       color='#8da0cb',\n",
    "                                                                       ax=ax8,\n",
    "                                                                       capsize=4,\n",
    "                                                                       xerr=[list(RF_importances_df.nlargest(15,\n",
    "                                                                                                             columns=\"mean_imp\",\n",
    "                                                                                                             keep='all')['low_CI']),\n",
    "                                                                             list(RF_importances_df.nlargest(15,\n",
    "                                                                                                             columns=\"mean_imp\",\n",
    "                                                                                                             keep='all')['high_CI'])],\n",
    "                                                                       label=\"Random Forest\")\n",
    "\n",
    "for tick in ax8.yaxis.get_major_ticks():\n",
    "     if tick.label1.get_text() in imp_words:\n",
    "         tick.label1.set_fontweight('bold')\n",
    "     else:\n",
    "         pass\n",
    "ax8.set_xlabel('Importance')\n",
    "ax8.grid(linestyle=':', axis='x')\n",
    "ax8.set_ylabel('')\n",
    "ax8.legend(loc='best', frameon=False)\n",
    "# ax8.set_title(\"Random Forest\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figure_path / 'fig1_importance_RF.pdf', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig9, ax9 = plt.subplots(figsize=plots.stdfigsize())\n",
    "\n",
    "XG_importances_df.nlargest(15,\n",
    "                           columns=\"mean_imp\",\n",
    "                           keep='all').sort_values(by='mean_imp').plot(x='feature',\n",
    "                                                                       y='mean_imp',\n",
    "                                                                       kind='barh',\n",
    "                                                                       color='#e78ac3',\n",
    "                                                                       ax=ax9,\n",
    "                                                                       capsize=4,\n",
    "                                                                       xerr=[list(XG_importances_df.nlargest(15,\n",
    "                                                                                                             columns=\"mean_imp\",\n",
    "                                                                                                             keep='all')['low_CI']),\n",
    "                                                                             list(XG_importances_df.nlargest(15,\n",
    "                                                                                                             columns=\"mean_imp\",\n",
    "                                                                                                             keep='all')['high_CI'])],\n",
    "                                                                       label=\"XGBoost\")\n",
    "\n",
    "for tick in ax9.yaxis.get_major_ticks():\n",
    "     if tick.label1.get_text() in imp_words:\n",
    "         tick.label1.set_fontweight('bold')\n",
    "     else:\n",
    "         pass\n",
    "ax9.set_xlabel('Importance')\n",
    "ax9.grid(linestyle=':', axis='x')\n",
    "ax9.set_ylabel('')\n",
    "ax9.legend(loc='best', frameon=False)\n",
    "# ax9.set_title(\"XGBoost\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figure_path / 'fig1_importance_XG.pdf', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig10, ax10 = plt.subplots(3, 2, figsize=plots.stdfigsize(nx=2, ny=3))\n",
    "\n",
    "ax10[0,0].plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\", alpha=0.8, label=\"No-skill line\")\n",
    "\n",
    "# ax10[0,0].plot(mean_fpr, mean_DT_tpr, color='#66c2a5', label=f\"Decision tree performance:\\n{mean_DT_auc:.2f}\")\n",
    "# ax10[0,0].fill_between(mean_fpr, DT_tpr_CI95[0], DT_tpr_CI95[1],\n",
    "#                 color='#66c2a5', alpha=0.2)\n",
    "\n",
    "# ax10[0,0].plot(mean_fpr, mean_LR_tpr, color='#fc8d62', label=\"Logistic regression\")\n",
    "# ax10[0,0].fill_between(mean_fpr, LR_tpr_CI95[0], LR_tpr_CI95[1],\n",
    "#                 color='#fc8d62', alpha=0.2)\n",
    "\n",
    "# ax10[0,0].plot(mean_fpr, mean_RF_tpr, color='#8da0cb', label=\"Random Forest\")\n",
    "# ax10[0,0].fill_between(mean_fpr, RF_tpr_CI95[0], RF_tpr_CI95[1],\n",
    "#                 color='#8da0cb', alpha=0.2)\n",
    "\n",
    "ax10[0,0].plot(mean_fpr, mean_XG_tpr, color='#e78ac3', label=f\"XGBoost\\nMean AUROC: {mean_XG_auc:.2f}\")\n",
    "ax10[0,0].fill_between(mean_fpr, XG_tpr_CI95[0], XG_tpr_CI95[1],\n",
    "                color='#e78ac3', alpha=0.2)\n",
    "\n",
    "ax10[0,0].set_xlabel(\"False Positive Rate\")\n",
    "ax10[0,0].set_ylabel(\"True Positive Rate\")\n",
    "ax10[0,0].set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax10[0,0].grid(linestyle=':')\n",
    "ax10[0,0].legend(loc='lower right', frameon=False, fontsize=20)\n",
    "ax10[0,0].text(-0.25, 1.05, \"a\", transform=ax10[0,0].transAxes,\n",
    "               fontsize=30, fontweight='bold', va='top')\n",
    "\n",
    "\n",
    "ax10[0,1].bar(models, heights, color=['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3'],\n",
    "        yerr=CI95, capsize=5)\n",
    "\n",
    "# ax10[0,1].set_xlabel('Classifier')\n",
    "ax10[0,1].set_ylabel('Performance (AUROC)')\n",
    "ax10[0,1].grid(linestyle=':', axis='y')\n",
    "ax10[0,1].set_ylim(0.5, 1.0)\n",
    "ax10[0,1].text(-0.25, 1.05, \"b\", transform=ax10[0,1].transAxes,\n",
    "               fontsize=30, fontweight='bold', va='top')\n",
    "\n",
    "\n",
    "DT_importances_df.nlargest(15,\n",
    "                           columns=\"mean_imp\",\n",
    "                           keep='all').sort_values(by='mean_imp').plot(x='feature',\n",
    "                                                                       y='mean_imp',\n",
    "                                                                       kind='barh',\n",
    "                                                                       color='#66c2a5',\n",
    "                                                                       ax=ax10[1,0],\n",
    "                                                                       capsize=4,\n",
    "                                                                       xerr=[list(DT_importances_df.nlargest(15,\n",
    "                                                                                                             columns=\"mean_imp\",\n",
    "                                                                                                             keep='all')['low_CI']),\n",
    "                                                                             list(DT_importances_df.nlargest(15,\n",
    "                                                                                                             columns=\"mean_imp\",\n",
    "                                                                                                             keep='all')['high_CI'])],\n",
    "                                                                       label='Decision Tree')\n",
    "\n",
    "for tick in ax10[1,0].yaxis.get_major_ticks():\n",
    "     if tick.label1.get_text() in imp_words:\n",
    "         tick.label1.set_fontweight('bold')\n",
    "     else:\n",
    "         pass\n",
    "ax10[1,0].set_xlabel('Importance')\n",
    "ax10[1,0].set_ylabel('')\n",
    "ax10[1,0].grid(linestyle=':', axis='x')\n",
    "ax10[1,0].legend(loc='best', frameon=False, markerscale=0.5, fontsize=20)\n",
    "ax10[1,0].text(-0.30, 1.05, \"c\", transform=ax10[1,0].transAxes,\n",
    "               fontsize=30, fontweight='bold', va='top')\n",
    "\n",
    "\n",
    "LR_imp.plot(x='feature',\n",
    "            y='mean_imp',\n",
    "            kind='barh',\n",
    "            color='#fc8d62',\n",
    "            ax=ax10[1,1],\n",
    "            capsize=4,\n",
    "            xerr=[list(LR_imp.nlargest(15,\n",
    "                                       columns=\"mean_imp\",\n",
    "                                       keep='all')['low_CI']),\n",
    "                  list(LR_imp.nlargest(15,\n",
    "                                       columns=\"mean_imp\",\n",
    "                                       keep='all')['high_CI'])],\n",
    "            label=\"Logistic\\nRegression\")\n",
    "\n",
    "for tick in ax10[1,1].yaxis.get_major_ticks():\n",
    "     if tick.label1.get_text() in imp_words:\n",
    "         tick.label1.set_fontweight('bold')\n",
    "     else:\n",
    "         pass\n",
    "ax10[1,1].set_xlabel('Importance')\n",
    "ax10[1,1].set_ylabel('')\n",
    "ax10[1,1].grid(linestyle=':', axis='x')\n",
    "ax10[1,1].legend(loc='best', frameon=False, markerscale=0.5)\n",
    "\n",
    "\n",
    "RF_importances_df.nlargest(15,\n",
    "                           columns=\"mean_imp\",\n",
    "                           keep='all').sort_values(by='mean_imp').plot(x='feature',\n",
    "                                                                       y='mean_imp',\n",
    "                                                                       kind='barh',\n",
    "                                                                       color='#8da0cb',\n",
    "                                                                       ax=ax10[2,0],\n",
    "                                                                       capsize=4,\n",
    "                                                                       xerr=[list(RF_importances_df.nlargest(15,\n",
    "                                                                                                             columns=\"mean_imp\",\n",
    "                                                                                                             keep='all')['low_CI']),\n",
    "                                                                             list(RF_importances_df.nlargest(15,\n",
    "                                                                                                             columns=\"mean_imp\",\n",
    "                                                                                                             keep='all')['high_CI'])],\n",
    "                                                                       label=\"Random Forest\")\n",
    "                           \n",
    "for tick in ax10[2,0].yaxis.get_major_ticks():\n",
    "     if tick.label1.get_text() in imp_words:\n",
    "         tick.label1.set_fontweight('bold')\n",
    "     else:\n",
    "         pass\n",
    "ax10[2,0].set_xlabel('Importance')\n",
    "ax10[2,0].grid(linestyle=':', axis='x')\n",
    "ax10[2,0].set_ylabel('')\n",
    "ax10[2,0].legend(loc='best', frameon=False, markerscale=0.5)\n",
    "\n",
    "\n",
    "XG_importances_df.nlargest(15,\n",
    "                           columns=\"mean_imp\",\n",
    "                           keep='all').sort_values(by='mean_imp').plot(x='feature',\n",
    "                                                                       y='mean_imp',\n",
    "                                                                       kind='barh',\n",
    "                                                                       color='#e78ac3',\n",
    "                                                                       ax=ax10[2,1],\n",
    "                                                                       capsize=4,\n",
    "                                                                       xerr=[list(XG_importances_df.nlargest(15,\n",
    "                                                                                                             columns=\"mean_imp\",\n",
    "                                                                                                             keep='all')['low_CI']),\n",
    "                                                                             list(XG_importances_df.nlargest(15,\n",
    "                                                                                                             columns=\"mean_imp\",\n",
    "                                                                                                             keep='all')['high_CI'])],\n",
    "                                                                       label=\"XGBoost\")\n",
    "\n",
    "for tick in ax10[2,1].yaxis.get_major_ticks():\n",
    "     if tick.label1.get_text() in imp_words:\n",
    "         tick.label1.set_fontweight('bold')\n",
    "     else:\n",
    "         pass\n",
    "ax10[2,1].set_xlabel('Importance')\n",
    "ax10[2,1].grid(linestyle=':', axis='x')\n",
    "ax10[2,1].set_ylabel('')\n",
    "ax10[2,1].legend(loc='best', frameon=False, markerscale=0.5)\n",
    "\n",
    "fig10.tight_layout()\n",
    "# plt.savefig(figure_path / 'fig1.png', dpi=1000)\n",
    "# plt.savefig(figure_path / 'fig1.pdf', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Plotting calibration curves with uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig11, ax11 = plt.subplots(2, 2, figsize=plots.stdfigsize(nx=2, ny=2))\n",
    "\n",
    "# Logistic Regression\n",
    "ax11[0,0].plot(\n",
    "    mean_mpv, mean_DT_fop, color='#66c2a5', marker=\"o\",\n",
    "    label=f\"Decision Tree\\nMean DW = {mean_DT_dws:.2f}\")\n",
    "ax11[0,0].fill_between(mean_mpv, DT_fop_CI95[0], DT_fop_CI95[1], color='#66c2a5', alpha=0.2)\n",
    "\n",
    "# Logistic Regression\n",
    "ax11[0,1].plot(\n",
    "    mean_mpv, mean_LR_fop, color='#fc8d62', marker=\"o\",\n",
    "    label=f\"Logistic Regression\\nMean DW = {mean_LR_dws:.2f}\")\n",
    "ax11[0,1].fill_between(mean_mpv, LR_fop_CI95[0], LR_fop_CI95[1], color='#fc8d62', alpha=0.2)\n",
    "\n",
    "# Random Forest\n",
    "ax11[1,0].plot(\n",
    "    mean_mpv, mean_RF_fop, color='#8da0cb', marker=\"o\",\n",
    "    label=f\"Random Forest\\nMean DW = {mean_RF_dws:.2f}\")\n",
    "ax11[1,0].fill_between(mean_mpv,RF_fop_CI95[0], RF_fop_CI95[1], color='#8da0cb', alpha=0.2)\n",
    "\n",
    "# XGBoost\n",
    "ax11[1,1].plot(\n",
    "    mean_mpv, mean_XG_fop, color='#e78ac3', marker=\"o\",\n",
    "    label=f\"XGBoost\\nMean DW = {mean_XG_dws:.2f}\")\n",
    "ax11[1,1].fill_between(mean_mpv, XG_fop_CI95[0], XG_fop_CI95[1], color='#e78ac3', alpha=0.2)\n",
    "\n",
    "# Plot properties\n",
    "ax11[0,0].plot(mean_mpv, mean_mpv, linestyle=\"--\", color=\"k\", alpha=0.8, label=\"Perfectly calibrated\")\n",
    "ax11[0,0].set_ylabel(\"Fraction of positive labels\")\n",
    "ax11[0,0].set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax11[0,0].grid(linestyle=':')\n",
    "ax11[0,0].legend(loc='best', frameon=False)\n",
    "ax11[0,0].text(-0.2, 1.05, \"a\", transform=ax11[0,0].transAxes,\n",
    "               fontsize=30, fontweight='bold', va='top')\n",
    "\n",
    "ax11[0,1].plot(mean_mpv, mean_mpv, linestyle=\"--\", color=\"k\", alpha=0.8)\n",
    "ax11[0,1].set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax11[0,1].grid(linestyle=':')\n",
    "ax11[0,1].legend(loc='best', frameon=False)\n",
    "ax11[0,1].text(-0.2, 1.05, \"b\", transform=ax11[0,1].transAxes,\n",
    "               fontsize=30, fontweight='bold', va='top')\n",
    "\n",
    "ax11[1,0].plot(mean_mpv, mean_mpv, linestyle=\"--\", color=\"k\", alpha=0.8)\n",
    "ax11[1,0].set_ylabel(\"Fraction of positive labels\")\n",
    "ax11[1,0].set_xlabel(\"Mean predicted probability\")\n",
    "ax11[1,0].set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax11[1,0].grid(linestyle=':')\n",
    "ax11[1,0].legend(loc='best', frameon=False)\n",
    "ax11[1,0].text(-0.2, 1.05, \"c\", transform=ax11[1,0].transAxes,\n",
    "               fontsize=30, fontweight='bold', va='top')\n",
    "\n",
    "ax11[1,1].plot(mean_mpv, mean_mpv, linestyle=\"--\", color=\"k\", alpha=0.8)\n",
    "ax11[1,1].set_xlabel(\"Mean predicted probability\")\n",
    "ax11[1,1].set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax11[1,1].grid(linestyle=':')\n",
    "ax11[1,1].legend(loc='best', frameon=False)\n",
    "ax11[1,1].text(-0.2, 1.05, \"d\", transform=ax11[1,1].transAxes,\n",
    "               fontsize=30, fontweight='bold', va='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figure_path / \"fig2.pdf\", dpi=1000)\n",
    "# plt.savefig(figure_path / \"fig2.png\", dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figu, eje = plt.subplots(2, 2, figsize=(15,15))\n",
    "\n",
    "# Decision Tree\n",
    "eje[0,0].hist(DT_preds, bins=20)\n",
    "\n",
    "eje[0,0].tick_params(axis='x', labelsize=18)\n",
    "eje[0,0].tick_params(axis='y', labelsize=18)\n",
    "eje[0,0].set_ylabel(\"Counts\", size=18)\n",
    "eje[0,0].set_title(\"Decision Tree\", size=20)\n",
    "eje[0,0].grid(linestyle=':')\n",
    "eje[0,0].set(ylim=[0, 225000])\n",
    "\n",
    "# Logistic Regression\n",
    "eje[0,1].hist(LR_preds, bins=20)\n",
    "\n",
    "eje[0,1].tick_params(axis='x', labelsize=18)\n",
    "eje[0,1].tick_params(axis='y', labelsize=18)\n",
    "eje[0,1].set_title(\"Logistic Regression\", size=20)\n",
    "eje[0,1].grid(linestyle=':')\n",
    "eje[0,1].set(ylim=[0, 225000])\n",
    "\n",
    "# Random Forest\n",
    "eje[1,0].hist(RF_preds, bins=20)\n",
    "\n",
    "eje[1,0].tick_params(axis='x', labelsize=18)\n",
    "eje[1,0].tick_params(axis='y', labelsize=18)\n",
    "eje[1,0].set_ylabel(\"Counts\", size=18)\n",
    "eje[1,0].set_xlabel(\"Estimated probabilities\", size=18)\n",
    "eje[1,0].set_title(\"Random Forest\", size=20)\n",
    "eje[1,0].grid(linestyle=':')\n",
    "eje[1,0].set(ylim=[0, 225000])\n",
    "\n",
    "# XGBoost\n",
    "eje[1,1].hist(XG_preds, bins=20)\n",
    "\n",
    "eje[1,1].tick_params(axis='x', labelsize=18)\n",
    "eje[1,1].tick_params(axis='y', labelsize=18)\n",
    "eje[1,1].set_xlabel(\"Estimated probabilities\", size=18)\n",
    "eje[1,1].set_title(\"XGBoost\", size=20)\n",
    "eje[1,1].grid(linestyle=':')\n",
    "eje[1,1].set(ylim=[0, 225000])\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figure_path / \"probability_histograms.png\", dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizability figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in Hospital B (2017-18)\n",
    "segmented_cohort3_jesse_curt = pd.read_csv(test_data3b)\n",
    "segmented_cohort3_three_annot = pd.read_csv(test_data3c)\n",
    "\n",
    "segmented_cohort3_jesse_curt['seg_cxr_text'] = segmented_cohort3_jesse_curt['seg_cxr_text'].str.replace(r\"'\", r\"\", regex=True)\n",
    "segmented_cohort3_jesse_curt['seg_cxr_text'] = segmented_cohort3_jesse_curt['seg_cxr_text'].str.replace(r\"\\[\", r\"\", regex=True)\n",
    "segmented_cohort3_jesse_curt['seg_cxr_text'] = segmented_cohort3_jesse_curt['seg_cxr_text'].str.replace(r\"\\]\", r\"\", regex=True)\n",
    "segmented_cohort3_jesse_curt['seg_cxr_text'] = segmented_cohort3_jesse_curt['seg_cxr_text'].str.replace(r\",\", r\"\", regex=True)\n",
    "\n",
    "segmented_cohort3_three_annot['seg_cxr_text'] = segmented_cohort3_three_annot['seg_cxr_text'].str.replace(r\"'\", r\"\", regex=True)\n",
    "segmented_cohort3_three_annot['seg_cxr_text'] = segmented_cohort3_three_annot['seg_cxr_text'].str.replace(r\"\\[\", r\"\", regex=True)\n",
    "segmented_cohort3_three_annot['seg_cxr_text'] = segmented_cohort3_three_annot['seg_cxr_text'].str.replace(r\"\\]\", r\"\", regex=True)\n",
    "segmented_cohort3_three_annot['seg_cxr_text'] = segmented_cohort3_three_annot['seg_cxr_text'].str.replace(r\",\", r\"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in MIMIC III\n",
    "mimic_iii = pd.read_csv(mimic3_path / \"cxr.csv\")\n",
    "\n",
    "# Removing remaining punctuation marks\n",
    "mimic_iii['seg_cxr_text'] = mimic_iii['seg_cxr_text'].str.replace(r\"'\", r\"\", regex=True)\n",
    "mimic_iii['seg_cxr_text'] = mimic_iii['seg_cxr_text'].str.replace(r\"\\[\", r\"\", regex=True)\n",
    "mimic_iii['seg_cxr_text'] = mimic_iii['seg_cxr_text'].str.replace(r\"\\]\", r\"\", regex=True)\n",
    "mimic_iii['seg_cxr_text'] = mimic_iii['seg_cxr_text'].str.replace(r\",\", r\"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hospital A (2013)\n",
    "X1 = segmented[cols[which][0]].to_numpy()\n",
    "Y1 = segmented[cols[which][1]].to_numpy()\n",
    "\n",
    "# # Accounting for potential label imbalance\n",
    "# count_0_Y1 = pd.Series(Y1).value_counts()[0]\n",
    "# count_1_Y1 = pd.Series(Y1).value_counts()[1]\n",
    "# weight_0_Y1 = count_0_Y1/ len(Y1)\n",
    "# weight_1_Y1 = count_1_Y1 / len(Y1)\n",
    "\n",
    "# Hospital A (2016)\n",
    "X2 = segmented_cohort2['segmented_report'].to_numpy()\n",
    "Y2 = segmented_cohort2['score'].to_numpy()\n",
    "\n",
    "# # Accounting for potential label imbalance\n",
    "# count_0_Y2 = pd.Series(Y2).value_counts()[0]\n",
    "# count_1_Y2 = pd.Series(Y2).value_counts()[1]\n",
    "# weight_0_Y2 = count_0_Y2 / len(Y2)\n",
    "# weight_1_Y2 = count_1_Y2 / len(Y2)\n",
    "\n",
    "# Hospital B (2017-18)\n",
    "X3_original = segmented_cohort3_original['seg_cxr_text'].values\n",
    "Y3_original = segmented_cohort3_original['score_final'].values\n",
    "X3_jesse_curt = segmented_cohort3_jesse_curt['seg_cxr_text'].values\n",
    "X3_three_annot = segmented_cohort3_three_annot['seg_cxr_text'].values\n",
    "\n",
    "# MIMIC III\n",
    "X4 = mimic_iii['seg_cxr_text'].to_numpy()\n",
    "Y4 = mimic_iii['curt_bl_infiltrates_(1=yes)'].to_numpy()\n",
    "\n",
    "which1 = 'a'\n",
    "cols1 = {\n",
    "    'a': ['seg_cxr_text', 'score_final'],\n",
    "    'b': ['seg_cxr_text', 'score']\n",
    "    }\n",
    "\n",
    "which2 = 'a'\n",
    "cols2 = {\n",
    "    'a': ['seg_cxr_text', 'curt_bl_infiltrates_(1=yes)'],\n",
    "    'b': ['seg_cxr_text', 'cxr_score_predicted']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get optimized hyperparameters for each of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(basedir / \"Development_notebooks\" /  \"hyperparameters\" / \"XG_hyperparams_hospital_a_2013.json\", \"r\") as xg_file:\n",
    "    xg_hyperparams = json.load(xg_file)\n",
    "\n",
    "xg_hyperparams['base_score'] = float(xg_hyperparams['base_score'])    \n",
    "xg_hyperparams['n_estimators'] = int(xg_hyperparams['n_estimators'])\n",
    "xg_hyperparams['max_depth'] = int(xg_hyperparams['max_depth'])\n",
    "xg_hyperparams['learning_rate'] = float(xg_hyperparams['learning_rate'])\n",
    "xg_hyperparams['gamma'] = float(xg_hyperparams['gamma'])\n",
    "xg_hyperparams['min_child_weight'] = float(xg_hyperparams['min_child_weight'])\n",
    "xg_hyperparams['max_delta_step'] = float(xg_hyperparams['max_delta_step'])\n",
    "xg_hyperparams['subsample'] = float(xg_hyperparams['subsample'])\n",
    "\n",
    "\n",
    "with open(basedir / \"Development_notebooks\" /  \"hyperparameters\" / \"XG_hyperparams_hospital_a_2016.json\", \"r\") as xg_file2:\n",
    "    xg_hyperparams2 = json.load(xg_file2)\n",
    "\n",
    "xg_hyperparams2['base_score'] = float(xg_hyperparams2['base_score']) \n",
    "xg_hyperparams2['n_estimators'] = int(xg_hyperparams2['n_estimators'])\n",
    "xg_hyperparams2['max_depth'] = int(xg_hyperparams2['max_depth'])\n",
    "xg_hyperparams2['learning_rate'] = float(xg_hyperparams2['learning_rate'])\n",
    "xg_hyperparams2['gamma'] = float(xg_hyperparams2['gamma'])\n",
    "xg_hyperparams2['min_child_weight'] = float(xg_hyperparams2['min_child_weight'])\n",
    "xg_hyperparams2['max_delta_step'] = float(xg_hyperparams2['max_delta_step'])\n",
    "xg_hyperparams2['subsample'] = float(xg_hyperparams2['subsample'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Hospital A (2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize\n",
    "vect1 = CountVectorizer(\n",
    "    tokenizer=tokenizer_better,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=200\n",
    "    )\n",
    "\n",
    "vect1.fit(X1)\n",
    "X1_vect_1 = vect1.transform(X1).toarray()\n",
    "\n",
    "XG_model_1 = XGBClassifier(\n",
    "    **xg_hyperparams,\n",
    "    tree_method='hist',\n",
    "    random_state=0\n",
    "    )\n",
    "\n",
    "XG_model_1.fit(X1_vect_1, Y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on Hospital A (2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprs_1_2 = []\n",
    "aucs_1_2 = []\n",
    "fops_1_2 = []\n",
    "preds_1_2 = []\n",
    "\n",
    "for i in tqdm(range(n_boot)):\n",
    "    # This line resamples the data, WITH replacement\n",
    "    boot_segmented_2 = segmented_cohort2.sample(\n",
    "        n=len(segmented_cohort2),\n",
    "        replace=True,\n",
    "        axis=0\n",
    "        )\n",
    "    \n",
    "    encounters = boot_segmented_2['_id'].unique()\n",
    "    \n",
    "    cv = KFold()\n",
    "    output1, output2, output3, output4, output5 = Parallel(n_jobs=5)(delayed(custom_cv_not_train)(\n",
    "        XG_model_1,\n",
    "        boot_segmented_2,\n",
    "        {'a': ['segmented_report', 'score']},\n",
    "        'a',\n",
    "        encounters,\n",
    "        vect1,\n",
    "        test_index,\n",
    "        mean_fpr,\n",
    "        mean_mpv\n",
    "        ) for test_index, _ in cv.split(encounters))\n",
    "    \n",
    "    aucs_1_2.append(np.mean([output1[0], output2[0], output3[0], output4[0], output5[0]]))\n",
    "    preds_1_2.extend(output1[1])\n",
    "    preds_1_2.extend(output2[1])\n",
    "    preds_1_2.extend(output3[1])\n",
    "    preds_1_2.extend(output4[1])\n",
    "    preds_1_2.extend(output5[1])\n",
    "    tprs_1_2.append(np.mean([output1[2], output2[2], output3[2], output4[2], output5[2]], axis=0))\n",
    "    fops_1_2.append(np.mean([output1[3], output2[3], output3[3], output4[3], output5[3]], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on Hospital B (2017-18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprs_1_3 = []\n",
    "aucs_1_3 = []\n",
    "fops_1_3 = []\n",
    "preds_1_3 = []\n",
    "\n",
    "for i in tqdm(range(n_boot)):\n",
    "    # This line resamples the data, WITH replacement\n",
    "    boot_segmented_3 = segmented_cohort3_original.sample(\n",
    "        n=len(segmented_cohort3_original),\n",
    "        replace=True,\n",
    "        axis=0\n",
    "        )\n",
    "    \n",
    "    encounters = boot_segmented_3['icu_id'].unique()\n",
    "    \n",
    "    cv = KFold()\n",
    "    output1, output2, output3, output4, output5 = Parallel(n_jobs=5)(delayed(custom_cv_not_train)(\n",
    "        XG_model_1,\n",
    "        boot_segmented_3,\n",
    "        cols1,\n",
    "        which1,\n",
    "        encounters,\n",
    "        vect1,\n",
    "        test_index,\n",
    "        mean_fpr,\n",
    "        mean_mpv\n",
    "        ) for test_index, _ in cv.split(encounters))\n",
    "    \n",
    "    aucs_1_3.append(np.mean([output1[0], output2[0], output3[0], output4[0], output5[0]]))\n",
    "    preds_1_3.extend(output1[1])\n",
    "    preds_1_3.extend(output2[1])\n",
    "    preds_1_3.extend(output3[1])\n",
    "    preds_1_3.extend(output4[1])\n",
    "    preds_1_3.extend(output5[1])\n",
    "    tprs_1_3.append(np.mean([output1[2], output2[2], output3[2], output4[2], output5[2]], axis=0))\n",
    "    fops_1_3.append(np.mean([output1[3], output2[3], output3[3], output4[3], output5[3]], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on a labeled subset of MIMIC III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprs_1_mimic = []\n",
    "aucs_1_mimic = []\n",
    "fops_1_mimic = []\n",
    "preds_1_mimic = []\n",
    "\n",
    "for i in tqdm(range(n_boot)):\n",
    "    # This line resamples the data, WITH replacement\n",
    "    boot_mimic3 = mimic_iii.sample(\n",
    "        n=len(mimic_iii),\n",
    "        replace=True,\n",
    "        axis=0\n",
    "        )\n",
    "    \n",
    "    encounters = boot_mimic3['encounter_id'].unique()\n",
    "    \n",
    "    cv = KFold()\n",
    "    output1, output2, output3, output4, output5 = Parallel(n_jobs=5)(delayed(custom_cv_not_train)(\n",
    "        XG_model_1,\n",
    "        boot_mimic3,\n",
    "        cols2,\n",
    "        which2,\n",
    "        encounters,\n",
    "        vect1,\n",
    "        test_index,\n",
    "        mean_fpr,\n",
    "        mean_mpv\n",
    "        ) for test_index, _ in cv.split(encounters))\n",
    "    \n",
    "    aucs_1_mimic.append(np.mean([output1[0], output2[0], output3[0], output4[0], output5[0]]))\n",
    "    preds_1_mimic.extend(output1[1])\n",
    "    preds_1_mimic.extend(output2[1])\n",
    "    preds_1_mimic.extend(output3[1])\n",
    "    preds_1_mimic.extend(output4[1])\n",
    "    preds_1_mimic.extend(output5[1])\n",
    "    tprs_1_mimic.append(np.mean([output1[2], output2[2], output3[2], output4[2], output5[2]], axis=0))\n",
    "    fops_1_mimic.append(np.mean([output1[3], output2[3], output3[3], output4[3], output5[3]], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Hospital A (2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XG2_imp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize\n",
    "vect2 = CountVectorizer(\n",
    "    tokenizer=tokenizer_better,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=200\n",
    "    )\n",
    "\n",
    "vect2.fit(X2)\n",
    "X2_vect_2 = vect2.transform(X2).toarray()\n",
    "features2 = {value: key for key, value in vect2.vocabulary_.items()}\n",
    "\n",
    "XG_model_2 = XGBClassifier(\n",
    "    **xg_hyperparams2,\n",
    "    tree_method='hist',\n",
    "    random_state=0\n",
    "    )\n",
    "    \n",
    "XG_model_2.fit(X2_vect_2, Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_imp_XG2 = XG_model_2.feature_importances_\n",
    "for i in range(len(raw_imp_XG2)):\n",
    "    temp = {'feature': features2[i], 'importance': raw_imp_XG2[i]}\n",
    "    XG2_imp.append(temp)\n",
    "    \n",
    "XG2_imp_df = pd.DataFrame(XG2_imp).sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figu, eje1 = plt.subplots(figsize=plots.stdfigsize())\n",
    "\n",
    "XG2_imp_df.nlargest(15,\n",
    "                    columns=\"importance\",\n",
    "                    keep='all').sort_values(by='importance').plot(x='feature',\n",
    "                                                                  y='importance',\n",
    "                                                                  kind='barh',\n",
    "                                                                  ax=eje1,\n",
    "                                                                  legend=False)\n",
    "\n",
    "eje1.set_xlabel('Normalized mean performance gain')\n",
    "eje1.grid(linestyle=':', axis='x')\n",
    "eje1.set_ylabel('')\n",
    "eje1.set_title(\"Importances - XGBoost trained on Hospital A (2016)\")\n",
    "figu.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on Hospital A (2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprs_2_1 = []\n",
    "aucs_2_1 = []\n",
    "fops_2_1 = []\n",
    "preds_2_1 = []\n",
    "\n",
    "for i in tqdm(range(n_boot)):\n",
    "    # This line resamples the data, WITH replacement\n",
    "    boot_segmented_1 = segmented.sample(\n",
    "        n=len(segmented),\n",
    "        replace=True,\n",
    "        axis=0\n",
    "        )\n",
    "    \n",
    "    encounters = boot_segmented_1['encounter_id'].unique()\n",
    "    \n",
    "    cv = KFold()\n",
    "    output1, output2, output3, output4, output5 = Parallel(n_jobs=5)(delayed(custom_cv_not_train)(\n",
    "        XG_model_2,\n",
    "        boot_segmented_1,\n",
    "        cols,\n",
    "        which,\n",
    "        encounters,\n",
    "        vect2,\n",
    "        test_index,\n",
    "        mean_fpr,\n",
    "        mean_mpv\n",
    "        ) for test_index, _ in cv.split(encounters))\n",
    "    \n",
    "    aucs_2_1.append(np.mean([output1[0], output2[0], output3[0], output4[0], output5[0]]))\n",
    "    preds_2_1.extend(output1[1])\n",
    "    preds_2_1.extend(output2[1])\n",
    "    preds_2_1.extend(output3[1])\n",
    "    preds_2_1.extend(output4[1])\n",
    "    preds_2_1.extend(output5[1])\n",
    "    tprs_2_1.append(np.mean([output1[2], output2[2], output3[2], output4[2], output5[2]], axis=0))\n",
    "    fops_2_1.append(np.mean([output1[3], output2[3], output3[3], output4[3], output5[3]], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on Hospital B (2017-18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprs_2_3 = []\n",
    "aucs_2_3 = []\n",
    "fops_2_3 = []\n",
    "preds_2_3 = []\n",
    "\n",
    "for i in tqdm(range(n_boot)):\n",
    "    # This line resamples the data, WITH replacement\n",
    "    boot_segmented_3 = segmented_cohort3_original.sample(\n",
    "        n=len(segmented_cohort3_original),\n",
    "        replace=True,\n",
    "        axis=0\n",
    "        )\n",
    "    \n",
    "    encounters = boot_segmented_3['icu_id'].unique()\n",
    "\n",
    "    cv = KFold()\n",
    "    output1, output2, output3, output4, output5 = Parallel(n_jobs=5)(delayed(custom_cv_not_train)(\n",
    "        XG_model_2,\n",
    "        boot_segmented_3,\n",
    "        cols1,\n",
    "        which1,\n",
    "        encounters,\n",
    "        vect2,\n",
    "        test_index,\n",
    "        mean_fpr,\n",
    "        mean_mpv\n",
    "        ) for test_index, _ in cv.split(encounters))\n",
    "    \n",
    "    aucs_2_3.append(np.mean([output1[0], output2[0], output3[0], output4[0], output5[0]]))\n",
    "    preds_2_3.extend(output1[1])\n",
    "    preds_2_3.extend(output2[1])\n",
    "    preds_2_3.extend(output3[1])\n",
    "    preds_2_3.extend(output4[1])\n",
    "    preds_2_3.extend(output5[1])\n",
    "    tprs_2_3.append(np.mean([output1[2], output2[2], output3[2], output4[2], output5[2]], axis=0))\n",
    "    fops_2_3.append(np.mean([output1[3], output2[3], output3[3], output4[3], output5[3]], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on a labeled subset of MIMIC III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprs_2_mimic = []\n",
    "aucs_2_mimic = []\n",
    "fops_2_mimic = []\n",
    "preds_2_mimic = []\n",
    "\n",
    "for i in tqdm(range(n_boot)):\n",
    "    # This line resamples the data, WITH replacement\n",
    "    boot_mimic3 = mimic_iii.sample(\n",
    "        n=len(mimic_iii),\n",
    "        replace=True,\n",
    "        axis=0\n",
    "        )\n",
    "    \n",
    "    encounters = boot_mimic3['encounter_id'].unique()\n",
    "    \n",
    "    cv = KFold()\n",
    "    output1, output2, output3, output4, output5 = Parallel(n_jobs=5)(delayed(custom_cv_not_train)(\n",
    "        XG_model_2,\n",
    "        boot_mimic3,\n",
    "        cols2,\n",
    "        which2,\n",
    "        encounters,\n",
    "        vect2,\n",
    "        test_index,\n",
    "        mean_fpr,\n",
    "        mean_mpv\n",
    "        ) for test_index, _ in cv.split(encounters))\n",
    "    \n",
    "    aucs_2_mimic.append(np.mean([output1[0], output2[0], output3[0], output4[0], output5[0]]))\n",
    "    preds_2_mimic.extend(output1[1])\n",
    "    preds_2_mimic.extend(output2[1])\n",
    "    preds_2_mimic.extend(output3[1])\n",
    "    preds_2_mimic.extend(output4[1])\n",
    "    preds_2_mimic.extend(output5[1])\n",
    "    tprs_2_mimic.append(np.mean([output1[2], output2[2], output3[2], output4[2], output5[2]], axis=0))\n",
    "    fops_2_mimic.append(np.mean([output1[3], output2[3], output3[3], output4[3], output5[3]], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hospital A (2013) on Hospital A (2016)\n",
    "mean_tpr_1_2 = np.mean(tprs_1_2, axis=0)\n",
    "mean_tpr_1_2[-1] = 1.0\n",
    "tpr_1_2_CI95 = [\n",
    "    np.percentile(tprs_1_2, 25, axis=0),\n",
    "    np.percentile(tprs_1_2, 75, axis=0)\n",
    "    ]\n",
    "\n",
    "mean_auc_1_2 = np.mean(aucs_1_2)\n",
    "auc_1_2_CI95 = [\n",
    "    np.percentile(aucs_1_2, 2.5),\n",
    "    np.percentile(aucs_1_2, 97.5)\n",
    "    ]\n",
    "\n",
    "\n",
    "# Hospital A (2013) on Hospital B (2017-18)\n",
    "mean_tpr_1_3 = np.mean(tprs_1_3, axis=0)\n",
    "mean_tpr_1_3[-1] = 1.0\n",
    "tpr_1_3_CI95 = [\n",
    "    np.percentile(tprs_1_3, 2.5, axis=0),\n",
    "    np.percentile(tprs_1_3, 97.5, axis=0)\n",
    "    ]\n",
    "\n",
    "mean_auc_1_3 = np.mean(aucs_1_3)\n",
    "auc_1_3_CI95 = [\n",
    "    np.percentile(aucs_1_3, 2.5),\n",
    "    np.percentile(aucs_1_3, 97.5)\n",
    "    ]\n",
    "\n",
    "\n",
    "# Hospital A (2013) on MIMIC-III\n",
    "mean_tpr_1_mimic = np.mean(tprs_1_mimic, axis=0)\n",
    "mean_tpr_1_mimic[-1] = 1.0\n",
    "tpr_1_mimic_CI95 = [\n",
    "    np.percentile(tprs_1_mimic, 2.5, axis=0),\n",
    "    np.percentile(tprs_1_mimic, 97.5, axis=0)\n",
    "    ]\n",
    "\n",
    "mean_auc_1_mimic = np.mean(aucs_1_mimic)\n",
    "auc_1_mimic_CI95 = [\n",
    "    np.percentile(aucs_1_mimic, 2.5),\n",
    "    np.percentile(aucs_1_mimic, 97.5)\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "# Hospital A (2016) on Hospital A (2013)\n",
    "mean_tpr_2_1 = np.mean(tprs_2_1, axis=0)\n",
    "mean_tpr_2_1[-1] = 1.0\n",
    "tpr_2_1_CI95 = [\n",
    "    np.percentile(tprs_2_1, 2.5, axis=0),\n",
    "    np.percentile(tprs_2_1, 97.5, axis=0)\n",
    "    ]\n",
    "\n",
    "mean_auc_2_1 = np.mean(aucs_2_1)\n",
    "auc_2_1_CI95 = [\n",
    "    np.percentile(aucs_2_1, 2.5),\n",
    "    np.percentile(aucs_2_1, 97.5)\n",
    "    ]\n",
    "\n",
    "\n",
    "# Hospital A (2016) on Hospital B (2017-18)\n",
    "mean_tpr_2_3 = np.mean(tprs_2_3, axis=0)\n",
    "mean_tpr_2_3[-1] = 1.0\n",
    "tpr_2_3_CI95 = [\n",
    "    np.percentile(tprs_2_3, 2.5, axis=0),\n",
    "    np.percentile(tprs_2_3, 97.5, axis=0)\n",
    "    ]\n",
    "\n",
    "mean_auc_2_3 = np.mean(aucs_2_3)\n",
    "auc_2_3_CI95 = [\n",
    "    np.percentile(aucs_2_3, 2.5),\n",
    "    np.percentile(aucs_2_3, 97.5)\n",
    "    ]\n",
    "\n",
    "# Hospital A (2016) on MIMIC-III\n",
    "mean_tpr_2_mimic = np.mean(tprs_2_mimic, axis=0)\n",
    "mean_tpr_2_mimic[-1] = 1.0\n",
    "tpr_2_mimic_CI95 = [\n",
    "    np.percentile(tprs_2_mimic, 2.5, axis=0),\n",
    "    np.percentile(tprs_2_mimic, 97.5, axis=0)\n",
    "    ]\n",
    "\n",
    "mean_auc_2_mimic = np.mean(aucs_2_mimic)\n",
    "auc_2_mimic_CI95 = [\n",
    "    np.percentile(aucs_2_mimic, 2.5),\n",
    "    np.percentile(aucs_2_mimic, 97.5)\n",
    "    ]\n",
    "\n",
    "# For AUC plot\n",
    "tests1 = [\n",
    "    'Hospital B (2017-18)',\n",
    "    'MIMIC (2001-12)'\n",
    "    ]\n",
    "heights1 = [mean_auc_1_3, mean_auc_1_mimic]\n",
    "CI951 = [\n",
    "    [heights1[0] - auc_1_3_CI95[0], heights1[1] - auc_1_mimic_CI95[0]],\n",
    "    [auc_1_3_CI95[1] - heights1[0], auc_1_mimic_CI95[1] - heights1[1]]\n",
    "]\n",
    "\n",
    "tests2 = [\n",
    "    'Hospital B (2017-18)',\n",
    "    'MIMIC (2001-12)'\n",
    "    ]\n",
    "heights2 = [mean_auc_2_3, mean_auc_2_mimic]\n",
    "CI952 = [\n",
    "    [heights2[0] - auc_2_3_CI95[0], heights2[1] - auc_2_mimic_CI95[0]],\n",
    "    [auc_2_3_CI95[1] - heights2[0], auc_2_mimic_CI95[1] - heights2[1]]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig12, ax12 = plt.subplots(figsize=(7,7))\n",
    "\n",
    "ax12.bar(tests1, heights1, yerr=CI951, capsize=5)\n",
    "ax12.set_xlabel(None)\n",
    "ax12.set_ylabel('Performance (AUROC)')\n",
    "ax12.set_title(\"XGBoost trained on Hospital A (2013)\")\n",
    "ax12.grid(linestyle=':', axis='y')\n",
    "ax12.set_ylim(0.5, 1.0)\n",
    "fig12.tight_layout()\n",
    "# plt.savefig(figure_path / 'SIfig2_new.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig13, ax13 = plt.subplots(figsize=(7,7))\n",
    "\n",
    "ax13.bar(tests2, heights2, yerr=CI952, capsize=5)\n",
    "ax13.set_xlabel(None)\n",
    "ax13.set_ylabel('Performance (AUROC)')\n",
    "ax13.set_title(\"XGBoost trained on Hospital A (2016)\")\n",
    "ax13.grid(linestyle=':', axis='y')\n",
    "ax13.set_ylim(0.5, 1.0)\n",
    "fig13.tight_layout()\n",
    "# plt.savefig(figure_path / 'SIfig2_train2_XGBoost_auc.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig14, ax14 = plt.subplots(1, 2, figsize=plots.stdfigsize(nx=2, ny=1))\n",
    "\n",
    "ax14[0].bar(tests1, heights1, yerr=CI951, capsize=5)\n",
    "ax14[0].set_xlabel(None)\n",
    "ax14[0].set_ylabel('Performance (AUROC)')\n",
    "ax14[0].set_title(\"XGBoost trained on Hospital A (2013)\")\n",
    "ax14[0].grid(linestyle=':', axis='y')\n",
    "ax14[0].set_ylim(0.5, 1.0)\n",
    "ax14[0].text(-0.2, 1.05, \"a\", transform=ax14[0].transAxes,\n",
    "               fontsize=30, fontweight='bold', va='top')\n",
    "\n",
    "ax14[1].bar(tests2, heights2, yerr=CI952, capsize=5)\n",
    "ax14[1].set_xlabel(None)\n",
    "ax14[1].set_title(\"XGBoost trained on Hospital A (2016)\")\n",
    "ax14[1].grid(linestyle=':', axis='y')\n",
    "ax14[1].set_ylim(0.5, 1.0)\n",
    "ax14[1].text(-0.2, 1.05, \"b\", transform=ax14[1].transAxes,\n",
    "               fontsize=30, fontweight='bold', va='top')\n",
    "\n",
    "fig14.tight_layout()\n",
    "# plt.savefig(figure_path / 'SIfig2.png', dpi=1000)\n",
    "# plt.savefig(figure_path / 'SIfig2.pdf', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibration plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig15, ax15 = plt.subplots(2, 2, figsize=(15,15))\n",
    "\n",
    "# Hospital A (2013) on Hospital A (2016)\n",
    "mean_fop_1_2 = np.mean(fops_1_2, axis=0)\n",
    "fop_1_2_CI95 = [np.percentile(fops_1_2, 2.5, axis=0),\n",
    "                np.percentile(fops_1_2, 97.5, axis=0)]\n",
    "\n",
    "ax15[0,0].plot(mean_mpv, mean_fop_1_2, marker=\"o\")\n",
    "ax15[0,0].fill_between(\n",
    "    mean_mpv, fop_1_2_CI95[0], fop_1_2_CI95[1], alpha=0.2)\n",
    "ax15[0,0].plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\", alpha=0.8)\n",
    "\n",
    "ax15[0,0].tick_params(axis='x', labelsize=18)\n",
    "ax15[0,0].tick_params(axis='y', labelsize=18)\n",
    "ax15[0,0].set_ylabel(\"True Positive Rate\", size=18)\n",
    "ax15[0,0].set_title(\"Train 1, Test 2\", size=16)\n",
    "ax15[0,0].set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax15[0,0].grid(linestyle=':')\n",
    "\n",
    "\n",
    "# Hospital A (2013) on Hospital B (2017-18)\n",
    "mean_fop_1_3 = np.mean(fops_1_3, axis=0)\n",
    "fop_1_3_CI95 = [np.percentile(fops_1_3, 2.5, axis=0),\n",
    "                np.percentile(fops_1_3, 97.5, axis=0)]\n",
    "\n",
    "ax15[0,1].plot(mean_mpv, mean_fop_1_3, marker=\"o\")\n",
    "ax15[0,1].fill_between(\n",
    "    mean_mpv, fop_1_3_CI95[0], fop_1_3_CI95[1], alpha=0.2)\n",
    "ax15[0,1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\", alpha=0.8)\n",
    "\n",
    "ax15[0,1].tick_params(axis='x', labelsize=18)\n",
    "ax15[0,1].tick_params(axis='y', labelsize=18)\n",
    "ax15[0,1].set_title(\"Train 1, Test 3\", size=16)\n",
    "ax15[0,1].set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax15[0,1].grid(linestyle=':')\n",
    "\n",
    "\n",
    "# Hospital A (2016) on Hospital A (2013)\n",
    "mean_fop_2_1 = np.mean(fops_2_1, axis=0)\n",
    "fop_2_1_CI95 = [np.percentile(fops_2_1, 2.5, axis=0),\n",
    "                np.percentile(fops_2_1, 97.5, axis=0)]\n",
    "\n",
    "ax15[1,0].plot(mean_mpv, mean_fop_2_1, marker=\"o\")\n",
    "ax15[1,0].fill_between(\n",
    "    mean_mpv, fop_2_1_CI95[0], fop_2_1_CI95[1], alpha=0.2)\n",
    "ax15[1,0].plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\", alpha=0.8)\n",
    "\n",
    "ax15[1,0].tick_params(axis='x', labelsize=18)\n",
    "ax15[1,0].tick_params(axis='y', labelsize=18)\n",
    "ax15[1,0].set_xlabel(\"Mean estimated probability\", size=18)\n",
    "ax15[1,0].set_ylabel(\"Fraction of positive label\", size=18)\n",
    "ax15[1,0].set_title(\"Train 2, Test 1\", size=16)\n",
    "ax15[1,0].set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax15[1,0].grid(linestyle=':')\n",
    "\n",
    "\n",
    "# Hospital A (2016) on Hospital B (2017-18)\n",
    "mean_fop_2_3 = np.mean(fops_2_3, axis=0)\n",
    "fop_2_3_CI95 = [\n",
    "    np.percentile(fops_2_3, 2.5, axis=0),\n",
    "    np.percentile(fops_2_3, 97.5, axis=0)\n",
    "    ]\n",
    "\n",
    "ax15[1,1].plot(mean_mpv, mean_fop_2_3, marker=\"o\")\n",
    "ax15[1,1].fill_between(\n",
    "    mean_mpv, fop_2_3_CI95[0], fop_2_3_CI95[1], alpha=0.2)\n",
    "ax15[1,1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\", alpha=0.8)\n",
    "\n",
    "ax15[1,1].tick_params(axis='x', labelsize=18)\n",
    "ax15[1,1].tick_params(axis='y', labelsize=18)\n",
    "ax15[1,1].set_xlabel(\"Mean estimated probability\", size=18)\n",
    "ax15[1,1].set_title(\"Train 2, Test 3\", size=16)\n",
    "ax15[1,1].set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax15[1,1].grid(linestyle=':')\n",
    "\n",
    "# plt.savefig(\"Calibration_generalization.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on MIMIC III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(basedir / \"Development_notebooks\" /  \"hyperparameters\" / \"bilateral_infiltrates_model_hyperparams.json\", \"r\") as bilat:\n",
    "    cxr_model_hyperparams = json.load(bilat)\n",
    "\n",
    "cxr_model_hyperparams['base_score'] = float(cxr_model_hyperparams['base_score'])    \n",
    "cxr_model_hyperparams['n_estimators'] = int(cxr_model_hyperparams['n_estimators'])\n",
    "cxr_model_hyperparams['max_depth'] = int(cxr_model_hyperparams['max_depth'])\n",
    "cxr_model_hyperparams['learning_rate'] = float(cxr_model_hyperparams['learning_rate'])\n",
    "cxr_model_hyperparams['gamma'] = float(cxr_model_hyperparams['gamma'])\n",
    "cxr_model_hyperparams['min_child_weight'] = float(cxr_model_hyperparams['min_child_weight'])\n",
    "cxr_model_hyperparams['max_delta_step'] = float(cxr_model_hyperparams['max_delta_step'])\n",
    "cxr_model_hyperparams['subsample'] = float(cxr_model_hyperparams['subsample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X5 = training_data[cols[which][0]].to_numpy()\n",
    "Y5 = training_data[cols[which][1]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize\n",
    "vect = CountVectorizer(\n",
    "    tokenizer=tokenizer_better,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=200\n",
    "    )\n",
    "\n",
    "vect.fit(X5)\n",
    "\n",
    "bilat_infilt_vect = vect.transform(X5).toarray()\n",
    "\n",
    "bilateral_infiltrates_model = XGBClassifier(\n",
    "    **cxr_model_hyperparams,\n",
    "    tree_method='hist',\n",
    "    random_state=0\n",
    "    )\n",
    "\n",
    "bilateral_infiltrates_model.fit(bilat_infilt_vect, Y5)\n",
    "\n",
    "# # Write vect and bilateral_infiltrates_model to disk\n",
    "# import pickle\n",
    "\n",
    "# with open(\"src/bilateral_infiltrates_model.pickle\", \"wb\") as bilat:\n",
    "#     pickle.dump(bilateral_infiltrates_model, bilat)\n",
    "    \n",
    "# with open(\"src/bilateral_infiltrates_model_vectorizer.pkl\", \"wb\") as bilat:\n",
    "#     pickle.dump(vect, bilat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprs_mimic = []\n",
    "aucs_mimic = []\n",
    "fops_mimic = []\n",
    "preds_mimic = []\n",
    "\n",
    "for i in tqdm(range(n_boot)):\n",
    "    # This line resamples the data, WITH replacement\n",
    "    boot_mimic3 = mimic_iii.sample(\n",
    "        n=len(mimic_iii),\n",
    "        replace=True,\n",
    "        axis=0\n",
    "        )\n",
    "    \n",
    "    encounters = boot_mimic3['encounter_id'].unique()\n",
    "    \n",
    "    cv = KFold()\n",
    "    output1, output2, output3, output4, output5 = Parallel(n_jobs=5)(delayed(custom_cv_not_train)(\n",
    "        bilateral_infiltrates_model,\n",
    "        boot_mimic3,\n",
    "        cols2,\n",
    "        which2,\n",
    "        encounters,\n",
    "        vect,\n",
    "        test_index,\n",
    "        mean_fpr,\n",
    "        mean_mpv\n",
    "        ) for test_index, _ in cv.split(encounters))\n",
    "    \n",
    "    aucs_mimic.append(np.mean([output1[0], output2[0], output3[0], output4[0], output5[0]]))\n",
    "    preds_mimic.extend(output1[1])\n",
    "    preds_mimic.extend(output2[1])\n",
    "    preds_mimic.extend(output3[1])\n",
    "    preds_mimic.extend(output4[1])\n",
    "    preds_mimic.extend(output5[1])\n",
    "    tprs_mimic.append(np.mean([output1[2], output2[2], output3[2], output4[2], output5[2]], axis=0))\n",
    "    fops_mimic.append(np.mean([output1[3], output2[3], output3[3], output4[3], output5[3]], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve - test bilateral infiltrates model on MIMIC III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tpr_mimic = np.mean(tprs_mimic, axis=0)\n",
    "mean_tpr_mimic[-1] = 1.0\n",
    "tpr_mimic_CI95 = [\n",
    "    np.percentile(tprs_mimic, 2.5, axis=0),\n",
    "    np.percentile(tprs_mimic, 97.5, axis=0)\n",
    "    ]\n",
    "\n",
    "mean_auc_mimic = np.mean(aucs_mimic)\n",
    "auc_mimic_CI95 = [\n",
    "    np.percentile(aucs_mimic, 2.5),\n",
    "    np.percentile(aucs_mimic, 97.5)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig16, ax16 = plt.subplots(figsize=(7,7))\n",
    "\n",
    "ax16.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr_mimic,\n",
    "    label=f\"Mean AUROC (95%CI): {mean_auc_mimic:.2f}\\n({auc_mimic_CI95[0]:.2f}-{auc_mimic_CI95[1]:.2f})\"\n",
    "    )\n",
    "ax16.fill_between(\n",
    "    mean_fpr,\n",
    "    tpr_mimic_CI95[0],\n",
    "    tpr_mimic_CI95[1],\n",
    "    alpha=0.2\n",
    "    )\n",
    "\n",
    "ax16.plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\", alpha=0.8)\n",
    "\n",
    "ax16.set_ylabel(\"True Positive Rate\")\n",
    "ax16.set_xlabel(\"False Positive Rate\")\n",
    "ax16.set_title(\"Bilateral infiltrates model on MIMIC (2001-12)\")\n",
    "ax16.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax16.grid(linestyle=':')\n",
    "ax16.legend(loc='best', frameon=False)\n",
    "\n",
    "fig16.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibration curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_fop_mimic = np.mean(fops_mimic, axis=0)\n",
    "fop_mimic_CI95 = [\n",
    "    np.percentile(fops_mimic, 2.5, axis=0),\n",
    "    np.percentile(fops_mimic, 97.5, axis=0)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig17, ax17 = plt.subplots(figsize=(7,7))\n",
    "\n",
    "ax17.plot(mean_mpv, mean_fop_mimic, marker=\"o\")\n",
    "ax17.fill_between(\n",
    "    mean_mpv, fop_mimic_CI95[0], fop_mimic_CI95[1], alpha=0.2)\n",
    "ax17.plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\", alpha=0.8)\n",
    "\n",
    "ax17.tick_params(axis='x', labelsize=18)\n",
    "ax17.tick_params(axis='y', labelsize=18)\n",
    "ax17.set_xlabel(\"Mean predicted probability\", size=18)\n",
    "ax17.set_ylabel(\"Fraction of positive labels\", size=18)\n",
    "ax17.set_title(\"Bilateral infiltrates model on MIMIC III\", size=16)\n",
    "ax17.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax17.grid(linestyle=':')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_mimic = vect.transform(X4).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.explainers.Tree(\n",
    "    bilateral_infiltrates_model,\n",
    "    vect_mimic,\n",
    "    feature_perturbation='interventional',\n",
    "    feature_names=vect.get_feature_names_out(),\n",
    "    model_output='probability'\n",
    "    )\n",
    "\n",
    "shap_values = explainer(vect_mimic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, max_display=15, show=False)\n",
    "\n",
    "axis1 = plt.gca()\n",
    "axis1.set_title(\"Bilateral Infiltrates Model on MIMIC (2001-12)\", fontsize=20)\n",
    "axis1.set_xlabel(None)\n",
    "axis1.tick_params(axis='both', which='both', labelsize=18)\n",
    "axis1.set_xlim(-0.5, 0.7)\n",
    "\n",
    "cb1 = plt.gcf().get_axes()[1]\n",
    "cb1.set_yticklabels(['0', str(vect_mimic.max())])\n",
    "cb1.set_ylabel(\"Word count in report\", size=20)\n",
    "cb1.tick_params(labelsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(figure_path / \"SIfig1.png\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.waterfall_plot(shap_values[25], max_display=15, show=False)\n",
    "\n",
    "axis5 = plt.gca()\n",
    "axis5.set_title(\"SHAP values for sample 25\", fontsize=20)\n",
    "\n",
    "for eje6 in plt.gcf().get_axes():\n",
    "    eje6.tick_params(axis='both', which='both', labelsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_iii.loc[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interrater agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig18, ax18 = plt.subplots(2,3, figsize=plots.stdfigsize(nx=3, ny=2))\n",
    "\n",
    "# Label frequency Hospital B (2017-18)\n",
    "ax18[0,0].hist(segmented_cohort3_original.score_final)\n",
    "ax18[0,0].set_ylabel(\"Counts\")\n",
    "ax18[0,0].set_title(\"Hospital B (2017-18) label frequency\")\n",
    "ax18[0,0].grid(linestyle=':')\n",
    "\n",
    "# Label frequency Hospital A (2013)\n",
    "ax18[0,1].hist(segmented.cxr_score)\n",
    "ax18[0,1].set_title(\"Hospital A (2013) label frequency\")\n",
    "ax18[0,1].grid(linestyle=':')\n",
    "\n",
    "# Hospital A (2013) on Hospital B (2017-18)\n",
    "ax18[0,2].hist(preds_1_3, bins=10)\n",
    "ax18[0,2].set_title(\"Train Hospital A (2013), Test Hospital B (2017-18) XGBoost probabilites\")\n",
    "ax18[0,2].grid(linestyle=':')\n",
    "\n",
    "\n",
    "# Label frequency Hospital B (2017-18)\n",
    "ax18[1,0].hist(segmented_cohort3_original.score_final)\n",
    "ax18[1,0].set_ylabel(\"Counts\")\n",
    "ax18[1,0].set_title(\"Hospital B (2017-18) label frequency\")\n",
    "ax18[1,0].set_xlabel(\"Labels\")\n",
    "ax18[1,0].grid(linestyle=':')\n",
    "\n",
    "# Label frequency Hospital A (2016)\n",
    "ax18[1,1].hist(segmented_cohort2.score)\n",
    "ax18[1,1].set_title(\"Hospital A (2016) label frequency\")\n",
    "ax18[1,1].set_xlabel(\"Labels\")\n",
    "ax18[1,1].grid(linestyle=':')\n",
    "\n",
    "# Hospital A (2016) on Hospital B (2017-18)\n",
    "ax18[1,2].hist(preds_2_3, bins=10)\n",
    "ax18[1,2].set_xlabel(\"Estimated probabilities\")\n",
    "ax18[1,2].set_title(\"Train Hospital A (2016), Test Hospital B (2017-18) XGBoost probabilites\")\n",
    "ax18[1,2].grid(linestyle=':')\n",
    "\n",
    "fig18.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like an easy explanation for XGBoost doing better in predicting Hospital B (2017-18) when trained on Hospital A (2016) is simply the greater similarity in class/label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1, l2 = 0.1, 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "mimic_iii['cxr_score_probability'] = bilateral_infiltrates_model.predict_proba(vect_mimic)[:, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraction disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting/bucketing table by XGBoost-outputted probability ranges\n",
    "# GOTTA SPECIFY WHICH COLUMNS TO DO THE DROPNA FOR\n",
    "agg_disagree = []\n",
    "r = mimic_iii['cxr_score_probability'] < l1\n",
    "s = mimic_iii['cxr_score_probability'] >= l1\n",
    "t = mimic_iii['cxr_score_probability'] < l2\n",
    "u = mimic_iii['cxr_score_probability'] >= l2\n",
    "\n",
    "for z in tqdm(range(n_boot)):\n",
    "    lows1 = mimic_iii.loc[r].sample(n=r.sum(), replace=True, axis=0)\n",
    "    intermediates1 = mimic_iii.loc[s&t].sample(n=(s&t).sum(), replace=True, axis=0)\n",
    "    highs1 = mimic_iii.loc[u].sample(n=u.sum(), replace=True, axis=0)\n",
    "    boot_mimic_iii = pd.concat([lows1, intermediates1, highs1], ignore_index=True)\n",
    "    \n",
    "    a5 = boot_mimic_iii['cxr_score_probability'] < l1\n",
    "    temp = boot_mimic_iii[a5].dropna(subset=['eryn_bl_infiltrates_(1=yes)', 'curt_bl_infiltrates_(1=yes)', 'seg_cxr_text'])\n",
    "    disagreements = 0\n",
    "    for idx, row in temp.iterrows():\n",
    "        disagreements += int(row[\"eryn_bl_infiltrates_(1=yes)\"] != row[\"curt_bl_infiltrates_(1=yes)\"])\n",
    "    \n",
    "    agg_disagree.append({'model_confidence': 'High\\nconfidence\\nNo', 'disagreement': disagreements/len(temp), 'model': \"Bilateral infiltrates model\"})\n",
    "\n",
    "\n",
    "    a6 = boot_mimic_iii['cxr_score_probability'] >= l1\n",
    "    a7 = boot_mimic_iii['cxr_score_probability'] < l2\n",
    "    temp = boot_mimic_iii[a6&a7].dropna(subset=['eryn_bl_infiltrates_(1=yes)', 'curt_bl_infiltrates_(1=yes)', 'seg_cxr_text'])\n",
    "    disagreements = 0\n",
    "    for idx, row in temp.iterrows():\n",
    "        disagreements += int(row[\"eryn_bl_infiltrates_(1=yes)\"] != row[\"curt_bl_infiltrates_(1=yes)\"])\n",
    "\n",
    "    agg_disagree.append({'model_confidence': \"Low confidence\", 'disagreement': disagreements/len(temp), 'model': \"Bilateral infiltrates model\"})\n",
    "\n",
    "\n",
    "    a8 = boot_mimic_iii['cxr_score_probability'] >= l2\n",
    "    temp = boot_mimic_iii[a8].dropna(subset=['eryn_bl_infiltrates_(1=yes)', 'curt_bl_infiltrates_(1=yes)', 'seg_cxr_text'])\n",
    "    disagreements = 0\n",
    "    for idx, row in temp.iterrows():\n",
    "        disagreements += int(row[\"eryn_bl_infiltrates_(1=yes)\"] != row[\"curt_bl_infiltrates_(1=yes)\"])\n",
    "\n",
    "    agg_disagree.append({'model_confidence': \"High\\nconfidence\\nYes\", 'disagreement': disagreements/len(temp), 'model': \"Bilateral infiltrates model\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement_df = pd.DataFrame(agg_disagree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement_df.groupby('model_confidence')['disagreement'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig19, ax19 = plt.subplots(figsize=plots.stdfigsize())\n",
    "\n",
    "sns.barplot(\n",
    "    x='model_confidence',\n",
    "    y='disagreement',\n",
    "    data=disagreement_df,\n",
    "    capsize=.2,\n",
    "    hue='model_confidence',\n",
    "    ax=ax19\n",
    "    )\n",
    "\n",
    "ax19.annotate(f\"n={r.sum()}\", (-0.20, 0.06))\n",
    "ax19.annotate(f\"n={(s&t).sum()}\", (0.80, 0.1))\n",
    "ax19.annotate(f\"n={u.sum()}\", (1.80, 0.17))\n",
    "\n",
    "ax19.set_ylabel(\"Mean fraction disagreed\")\n",
    "ax19.grid(linestyle=':', axis='y')\n",
    "ax19.set_xlabel(\"\")\n",
    "ax19.set_ylim(0, 0.4)\n",
    "# ax22.legend()\n",
    "\n",
    "fig19.tight_layout()\n",
    "# plt.savefig(figure_path / 'fig3_disagreement.png', dpi=1000)\n",
    "# plt.savefig(figure_path / 'fig3_disagreement.pdf', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mimic_iii['predicted'] = mimic_iii['cxr_score_probability'] >= threshold\n",
    "# If threshold == 0.5, the line above and the line below do the exact same thing\n",
    "mimic_iii['predicted'] = bilateral_infiltrates_model.predict(vect_mimic)\n",
    "\n",
    "cf = confusion_matrix(Y4, mimic_iii['predicted'])\n",
    "\n",
    "strings = np.asarray([['True negatives\\n', 'False positives\\n'],\n",
    "                      ['False negatives\\n', 'True positives\\n']])\n",
    "\n",
    "labels = (np.asarray([\"{0} {1:.0f}\".format(string, value)\n",
    "                      for string, value in zip(strings.flatten(),\n",
    "                                               cf.flatten())])\n",
    "         ).reshape(2, 2)\n",
    "\n",
    "fig20, ax20 = plt.subplots(figsize=plots.stdfigsize())\n",
    "sns.heatmap(cf, fmt='', annot=labels, cmap='Blues', cbar=False, ax=ax20)\n",
    "ax20.set_ylabel(\"Ground truth\")\n",
    "ax20.set_xlabel(\"Bilateral Infiltrates Model adjudicated\")\n",
    "ax20.tick_params(axis='both', bottom=False, left=False,\n",
    "                labelbottom=False, labelleft=False)\n",
    "\n",
    "fig20.tight_layout()\n",
    "# plt.savefig(figure_path / 'fig3_cf.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig21, ax21 = plt.subplots(2, 2, figsize=plots.stdfigsize(nx=2, ny=2))\n",
    "\n",
    "# ROC curve\n",
    "ax21[0,0].plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr_mimic,\n",
    "    label=f\"Mean AUROC: {mean_auc_mimic:.2f}\"\n",
    "    )\n",
    "ax21[0,0].fill_between(\n",
    "    mean_fpr,\n",
    "    tpr_mimic_CI95[0],\n",
    "    tpr_mimic_CI95[1],\n",
    "    alpha=0.2\n",
    "    )\n",
    "ax21[0,0].plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\", alpha=0.8, label=\"No-skill line\")\n",
    "\n",
    "# Calibration curve\n",
    "ax21[0,1].plot(mean_mpv, mean_fop_mimic, marker=\"o\")\n",
    "ax21[0,1].fill_between(\n",
    "    mean_mpv, fop_mimic_CI95[0], fop_mimic_CI95[1], alpha=0.2)\n",
    "ax21[0,1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\", alpha=0.8, label=\"Perfectly calibrated\")\n",
    "\n",
    "# Confusion matrix\n",
    "sns.heatmap(cf, fmt='', annot=labels, cmap='Blues', cbar=False, ax=ax21[1,0])\n",
    "\n",
    "# Interrater disagreement\n",
    "sns.barplot(x='model_confidence', y='disagreement', hue='model_confidence',\n",
    "            data=disagreement_df, capsize=.2, ax=ax21[1,1])\n",
    "\n",
    "# Plot properties\n",
    "ax21[0,0].set_ylabel(\"True Positive Rate\")\n",
    "ax21[0,0].set_xlabel(\"False Positive Rate\")\n",
    "ax21[0,0].set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax21[0,0].grid(linestyle=':')\n",
    "ax21[0,0].legend(loc='best', frameon=False)\n",
    "ax21[0,0].text(-0.2, 1.05, \"a\", transform=ax21[0,0].transAxes,\n",
    "               fontsize=30, fontweight='bold', va='top')\n",
    "\n",
    "ax21[0,1].tick_params(axis='x', labelsize=18)\n",
    "ax21[0,1].tick_params(axis='y', labelsize=18)\n",
    "ax21[0,1].set_xlabel(\"Mean predicted probability\")\n",
    "ax21[0,1].set_ylabel(\"Fraction of positive labels\")\n",
    "ax21[0,1].set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax21[0,1].grid(linestyle=':')\n",
    "ax21[0,1].text(-0.2, 1.05, \"b\", transform=ax21[0,1].transAxes,\n",
    "               fontsize=30, fontweight='bold', va='top')\n",
    "\n",
    "ax21[1,0].set_ylabel(\"Ground truth\")\n",
    "ax21[1,0].set_xlabel(\"Bilateral Infiltrates Model adjudicated\")\n",
    "ax21[1,0].tick_params(axis='both', bottom=False, left=False,\n",
    "                labelbottom=False, labelleft=False)\n",
    "ax21[1,0].text(-0.2, 1.05, \"c\", transform=ax21[1,0].transAxes,\n",
    "               fontsize=30, fontweight='bold', va='top')\n",
    "\n",
    "ax21[1,1].annotate(f\"n={r.sum()}\", (-0.20, 0.06))\n",
    "ax21[1,1].annotate(f\"n={(s&t).sum()}\", (0.8, 0.10))\n",
    "ax21[1,1].annotate(f\"n={u.sum()}\", (1.8, 0.17))\n",
    "ax21[1,1].set_ylabel(\"Mean fraction disagreed\")\n",
    "ax21[1,1].grid(linestyle=':', axis='y')\n",
    "ax21[1,1].set_xlabel(\"\")\n",
    "ax21[1,1].set_ylim(0.0, 0.4)\n",
    "ax21[1,1].text(-0.2, 1.05, \"d\", transform=ax21[1,1].transAxes,\n",
    "               fontsize=30, fontweight='bold', va='top')\n",
    "\n",
    "\n",
    "fig21.tight_layout()\n",
    "# plt.savefig(figure_path / \"fig3.png\", dpi=1000)\n",
    "# plt.savefig(figure_path / \"fig3.pdf\", dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SI Figure on different choices for cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different thresholds for the probability and see how the F1 score changes\n",
    "f1_scores = []\n",
    "f1_thresholds = np.linspace(0, 1, 1000)\n",
    "\n",
    "for threshold in f1_thresholds:\n",
    "    predictions = mimic_iii['cxr_score_probability'] >= threshold\n",
    "    f1_scores.append(f1_score(mimic_iii['curt_bl_infiltrates_(1=yes)'], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index of maximum value of scores\n",
    "max_f1_score = max(f1_scores)\n",
    "max_index = f1_scores.index(max_f1_score)\n",
    "max_f1_threshold = f1_thresholds[max_index]\n",
    "print(f\"Maximum F1 Score: {max_f1_score} at threshold: {max_f1_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_iii['predicted_f1'] = mimic_iii['cxr_score_probability'] >= max_f1_threshold\n",
    "\n",
    "cf_f1 = confusion_matrix(mimic_iii['curt_bl_infiltrates_(1=yes)'], mimic_iii['predicted_f1'])\n",
    "\n",
    "strings = np.asarray([['True negatives\\n', 'False positives\\n'],\n",
    "                      ['False negatives\\n', 'True positives\\n']])\n",
    "\n",
    "labels_f1 = (np.asarray([\"{0} {1:.0f}\".format(string, value)\n",
    "                      for string, value in zip(strings.flatten(),\n",
    "                                               cf_f1.flatten())])\n",
    "             ).reshape(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different thresholds for the probability and see how the accuracy changes\n",
    "accuracy_scores = []\n",
    "accuracy_thresholds = np.linspace(0, 1, 1000)\n",
    "\n",
    "for threshold in accuracy_thresholds:\n",
    "    predictions = mimic_iii['cxr_score_probability'] >= threshold\n",
    "    accuracy_scores.append(accuracy_score(mimic_iii['curt_bl_infiltrates_(1=yes)'], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index of maximum value of scores\n",
    "max_accuracy_score = max(accuracy_scores)\n",
    "max_index = accuracy_scores.index(max_accuracy_score)\n",
    "max_accuracy_threshold = accuracy_thresholds[max_index]\n",
    "print(f\"Maximum Accuracy: {max_accuracy_score} at threshold: {max_accuracy_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_iii['predicted_accuracy'] = mimic_iii['cxr_score_probability'] >= max_accuracy_threshold\n",
    "\n",
    "cf_accuracy = confusion_matrix(mimic_iii['curt_bl_infiltrates_(1=yes)'], mimic_iii['predicted_accuracy'])\n",
    "\n",
    "strings = np.asarray([['True negatives\\n', 'False positives\\n'],\n",
    "                      ['False negatives\\n', 'True positives\\n']])\n",
    "\n",
    "labels_accuracy = (np.asarray([\"{0} {1:.0f}\".format(string, value)\n",
    "                      for string, value in zip(strings.flatten(),\n",
    "                                               cf_accuracy.flatten())])\n",
    "                   ).reshape(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig22, ax22 = plt.subplots(1, 2, figsize=plots.stdfigsize(nx=2, ny=1))\n",
    "\n",
    "sns.heatmap(cf_f1, fmt='', annot=labels_f1, cmap='Blues', cbar=False, ax=ax22[0])\n",
    "ax22[0].set_ylabel(\"Ground truth\")\n",
    "ax22[0].set_xlabel(\"Bilateral Infiltrates Model adjudicated\")\n",
    "ax22[0].set_title(\"Optimal F1 score\")\n",
    "ax22[0].tick_params(axis='both', bottom=False, left=False,\n",
    "                labelbottom=False, labelleft=False)\n",
    "ax22[0].text(-0.2, 1.05, \"a\", transform=ax22[0].transAxes,\n",
    "               fontsize=30, fontweight='bold', va='top')\n",
    "\n",
    "sns.heatmap(cf_accuracy, fmt='', annot=labels_accuracy, cmap='Blues', cbar=False, ax=ax22[1])\n",
    "ax22[1].set_ylabel(\"Ground truth\")\n",
    "ax22[1].set_xlabel(\"Bilateral Infiltrates Model adjudicated\")\n",
    "ax22[1].set_title(\"Optimal accuracy\")\n",
    "ax22[1].tick_params(axis='both', bottom=False, left=False,\n",
    "                labelbottom=False, labelleft=False)\n",
    "ax22[1].text(-0.2, 1.05, \"b\", transform=ax22[1].transAxes,\n",
    "               fontsize=30, fontweight='bold', va='top')\n",
    "\n",
    "fig22.tight_layout()\n",
    "# plt.savefig(figure_path / 'SIfig3.png', dpi=1000)\n",
    "# plt.savefig(figure_path / 'SIfig3.pdf', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How would all of this look when raters agree vs. when they disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_iii_agree = mimic_iii.loc[(mimic_iii['curt_bl_infiltrates_(1=yes)'] == mimic_iii['eryn_bl_infiltrates_(1=yes)'])]\n",
    "mimic_iii_disagree = mimic_iii.loc[(mimic_iii['curt_bl_infiltrates_(1=yes)'] != mimic_iii['eryn_bl_infiltrates_(1=yes)'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mimic_iii_agree), len(mimic_iii_disagree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AUROC agree: {roc_auc_score(mimic_iii_agree['curt_bl_infiltrates_(1=yes)'], mimic_iii_agree['cxr_score_probability'])}\")\n",
    "print(f\"AUROC disagree: {roc_auc_score(mimic_iii_disagree['curt_bl_infiltrates_(1=yes)'], mimic_iii_disagree['cxr_score_probability'])}\")\n",
    "\n",
    "print(f\"Accuracy agree: {accuracy_score(mimic_iii_agree['curt_bl_infiltrates_(1=yes)'], mimic_iii_agree['predicted'])}\")\n",
    "print(f\"Accuracy disagree: {accuracy_score(mimic_iii_disagree['curt_bl_infiltrates_(1=yes)'], mimic_iii_disagree['predicted'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibrationDisplay\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(mimic_iii_agree['curt_bl_infiltrates_(1=yes)'], mimic_iii_agree['cxr_score_probability'], n_bins=10)\n",
    "CalibrationDisplay(prob_true, prob_pred, mimic_iii_agree['cxr_score_probability']).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_true, prob_pred = calibration_curve(mimic_iii_disagree['curt_bl_infiltrates_(1=yes)'], mimic_iii_disagree['cxr_score_probability'], n_bins=10)\n",
    "CalibrationDisplay(prob_true, prob_pred, mimic_iii_disagree['cxr_score_probability']).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real deal: the interrater disagreement graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_conf = mimic_iii.loc[(mimic_iii['cxr_score_probability'] >= l1) & (mimic_iii['cxr_score_probability'] < l2)]\n",
    "high_conf_no = mimic_iii.loc[mimic_iii['cxr_score_probability'] < l1]\n",
    "high_conf_yes = mimic_iii.loc[mimic_iii['cxr_score_probability'] >= l2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(low_conf), len(high_conf_no), len(high_conf_yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy high confidence no: {accuracy_score(high_conf_no['curt_bl_infiltrates_(1=yes)'], high_conf_no['predicted'])}\")\n",
    "print(f\"Accuracy low confidence: {accuracy_score(low_conf['curt_bl_infiltrates_(1=yes)'], low_conf['predicted'])}\")\n",
    "print(f\"Accuracy high confidence yes: {accuracy_score(high_conf_yes['curt_bl_infiltrates_(1=yes)'], high_conf_yes['predicted'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I remember looking at the reports, and wondering about how frequently would the model give a low probability to a report that was rated \"yes\" by both raters. Or a high probability to a report that was rated \"no\" by both raters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I remember looking at the reports, and wondering about how frequently would the model give a low probability to a report that was rated \"yes\" by both raters. Or a high probability to a report that was rated \"no\" by both raters. This is the graph that answers that question.\n",
    "low_conf_agree = mimic_iii_agree.loc[(mimic_iii_agree['cxr_score_probability'] >= l1) & (mimic_iii_agree['cxr_score_probability'] < l2)]\n",
    "high_conf_no_agree = mimic_iii_agree.loc[mimic_iii_agree['cxr_score_probability'] < l1]\n",
    "high_conf_yes_agree = mimic_iii_agree.loc[mimic_iii_agree['cxr_score_probability'] >= l2]\n",
    "\n",
    "low_conf_disagree = mimic_iii_disagree.loc[(mimic_iii_disagree['cxr_score_probability'] >= l1) & (mimic_iii_disagree['cxr_score_probability'] < l2)]\n",
    "high_conf_no_disagree = mimic_iii_disagree.loc[mimic_iii_disagree['cxr_score_probability'] < l1]\n",
    "high_conf_yes_disagree = mimic_iii_disagree.loc[mimic_iii_disagree['cxr_score_probability'] >= l2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy high confidence no: {accuracy_score(high_conf_no_agree['curt_bl_infiltrates_(1=yes)'], high_conf_no_agree['predicted'])}\")\n",
    "print(f\"Accuracy low confidence: {accuracy_score(low_conf_agree['curt_bl_infiltrates_(1=yes)'], low_conf_agree['predicted'])}\")\n",
    "print(f\"Accuracy high confidence yes: {accuracy_score(high_conf_yes_agree['curt_bl_infiltrates_(1=yes)'], high_conf_yes_agree['predicted'])}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Accuracy high confidence no: {accuracy_score(high_conf_no_disagree['curt_bl_infiltrates_(1=yes)'], high_conf_no_disagree['predicted'])}\")\n",
    "print(f\"Accuracy low confidence: {accuracy_score(low_conf_disagree['curt_bl_infiltrates_(1=yes)'], low_conf_disagree['predicted'])}\")\n",
    "print(f\"Accuracy high confidence yes: {accuracy_score(high_conf_yes_disagree['curt_bl_infiltrates_(1=yes)'], high_conf_yes_disagree['predicted'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
