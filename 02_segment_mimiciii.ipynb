{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f2b5627",
   "metadata": {},
   "source": [
    "# This notebook segments (i.e., breaks reports into key statements/words) tables containing report data for MIMIC III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3734524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44441a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.segmentation_tools import (\n",
    "    curate_indicator_word_list,\n",
    "    remove_easy_sections,\n",
    "    handle_subsection_titles,\n",
    "    remove_lines_on_other_organs,\n",
    "    stem_indicator_words,\n",
    "    remove_stopwords,\n",
    "    remove_sections_n_duplicate_lines,\n",
    "    refine_cleaning,\n",
    "    remove_dictation,\n",
    "    extract_surroundings_of_risk_factor_and_process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abec469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom display of tables for easier inspection\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "416fe5b4",
   "metadata": {},
   "source": [
    "This notebook aims to process datasets from MIMIC III for downstream automated ARDS/control adjudication.  \n",
    "Specifically, this notebook follows steps highlighted in bold-face:  \n",
    "- File I/O, which depends on a specs file.\n",
    "- Standard preprocessing, in which column names are standardized.  \n",
    "- Hospital-specific processing, which is temporarily custom-made.  \n",
    "- Anonymization. It follows two substeps:  \n",
    "    + Anonymizing patient/encounter IDs and datetime columns for all tables.\n",
    "    + Anonymizing text-based tables (chest X-ray reports, attending notes, ECHO reports). \n",
    "- **Segmentation of text-based tables.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04c15fd0",
   "metadata": {},
   "source": [
    "## Read in the tables and converting the text-based ones into list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383293c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = Path(\"..\")\n",
    "anonymized_location = basedir / 'Preprocessed_Data'\n",
    "cohort = 'MIMIC_III'\n",
    "path = anonymized_location / cohort / 'labeled_subset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c660e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = pd.read_csv(path / \"pf_ratio.csv\")\n",
    "pf['pf_ratio_timestamp'] = pd.to_datetime(pf['pf_ratio_timestamp'])\n",
    "pf['vent_start_timestamp'] = pd.to_datetime(pf['vent_start_timestamp'])\n",
    "\n",
    "try:\n",
    "    peep = pd.read_csv(path / \"peep.csv\")\n",
    "    peep['peep_timestamp'] = pd.to_datetime(peep['peep_timestamp'])\n",
    "except FileNotFoundError:\n",
    "    peep = None\n",
    "    print(\"This dataset doesn't seem to have peep separately specified.\")\n",
    "\n",
    "cxr = pd.read_csv(path / \"cxr.csv\")\n",
    "cxr['cxr_timestamp'] = pd.to_datetime(cxr['cxr_timestamp'])\n",
    "\n",
    "notes = pd.read_csv(path / \"attending_notes.csv\")\n",
    "notes['notes_timestamp'] = pd.to_datetime(notes['notes_timestamp'])\n",
    "\n",
    "echo = pd.read_csv(path / \"echo_reports.csv\")\n",
    "echo['echo_timestamp'] = pd.to_datetime(echo['echo_timestamp'])\n",
    "\n",
    "bnp = pd.read_csv(path / \"bnp.csv\")\n",
    "bnp['bnp_timestamp'] = pd.to_datetime(bnp['bnp_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af7799",
   "metadata": {},
   "outputs": [],
   "source": [
    "cxr_list = cxr.to_dict(orient='records')\n",
    "notes_list = notes.to_dict(orient='records')\n",
    "echo_list = echo.to_dict(orient='records')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfd046ee",
   "metadata": {},
   "source": [
    "## Defining parameters for segmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does it actually matter listing different sections for the cohorts?\n",
    "# The pattern of True/False is the same, but with new sections.\n",
    "# Might as well have just one list of tuples with comprehensive sections\n",
    "# found across every cohort. If a section is not in the CXRs of one\n",
    "# particular cohort, it would simply not match, right?\n",
    "section_order = {}\n",
    "\n",
    "section_order['mc1_t1'] = [\n",
    "    ('result:', True),\n",
    "    ('study:', False),\n",
    "    ('procedure:', False),\n",
    "    ('indication:', False),\n",
    "    ('technique:', False),\n",
    "    ('history:', False),\n",
    "    ('exam:', False),\n",
    "    ('comparison:', False),\n",
    "    ('finding_conclusion:', True),\n",
    "    ('finding:', True),\n",
    "    ('impression:', True),\n",
    "    ('conclusion:', True)\n",
    "    ]\n",
    "\n",
    "section_order['mc1_t2'] = [\n",
    "    ('procedure:', False),\n",
    "    ('indication:', False),\n",
    "    ('technique:', False),\n",
    "    ('history:', False),\n",
    "    ('exam:', False),\n",
    "    ('comparison:', False),\n",
    "    ('finding_conclusion:', True),\n",
    "    ('finding:', True),\n",
    "    ('impression:', True),\n",
    "    ('conclusion:', True)\n",
    "    ]\n",
    "\n",
    "\n",
    "section_order['mc2_t3'] = [\n",
    "    ('procedure:', False),\n",
    "    ('indication:', False),\n",
    "    ('technique:', False),\n",
    "    ('history:', False),\n",
    "    ('exam:', False),\n",
    "    ('comparison:', False),\n",
    "    ('finding_conclusion:', True),\n",
    "    ('finding:', True),\n",
    "    ('impression:', True),\n",
    "    ('conclusion:', True)\n",
    "    ]\n",
    "\n",
    "section_order['mimic_iii'] = [\n",
    "    ('result:', True),\n",
    "    ('study:', False),\n",
    "    ('procedure:', False),\n",
    "    ('indication:', False),\n",
    "    ('technique:', False),\n",
    "    ('history:', False),\n",
    "    ('exam:', False),\n",
    "    ('comparison:', False),\n",
    "    ('finding_conclusion:', True),\n",
    "    ('finding:', True),\n",
    "    ('impression:', True),\n",
    "    ('conclusion:', True)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba0e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words listed in exclusion_set mark statements in the report that do not address the lungs.\n",
    "#\n",
    "# Note: Curt recommended removing 'hila', 'hilar' from this list.\n",
    "#\n",
    "# They suggested that it may also be appropriate to remove 'venous'\n",
    "# but it requires further consideration.\n",
    "\n",
    "exclusion_set = {\n",
    "    'adenopathy', 'artery', 'aortic', 'atria',\n",
    "    'biliary', 'bowel', 'bones', 'cabg', 'carina',\n",
    "    'cardiac', 'cardiomegaly', 'catheter', 'chest',\n",
    "    'cirrhosis', 'devices', 'drain', 'drains', 'ett',\n",
    "    'gallbladder', 'heart', 'hearts', 'hydronephrosis',\n",
    "    'kidney', 'line', 'lines', 'liver', 'lymph',\n",
    "    'mediastinal', 'mediastinum', 'myeloma', 'picc',\n",
    "    'pneumomediastinum', 'spine', 'spleen',\n",
    "    'support_devices', 'tube', 'tubes', 'tubes_devices',\n",
    "    'vasculature', 'vein', 'vena', 'venous', 'ventric',\n",
    "    'wire', 'wires'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814d7037",
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_stemming = {\n",
    "    'bilaterally': 'bilateral',\n",
    "    'infiltrates': 'infiltrate',\n",
    "    'inhalational': 'inhalation',\n",
    "    'opacities': 'opacity',\n",
    "    'angles': 'angle',\n",
    "    'effusions': 'effusion',\n",
    "    'patches': 'patch',\n",
    "    'patchy': 'patch',\n",
    "    'spaces': 'space',\n",
    "    'traces': 'trace'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c681c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_stopwords = [\n",
    "    'there is ',\n",
    "    'there are ',\n",
    "    'there has been ',\n",
    "    'at this time'\n",
    "    ]\n",
    "\n",
    "simple_stopwords = [\n",
    "    'a', 'an', 'are', 'demonstrate',\n",
    "    'demonstrated', 'is', 'noted',\n",
    "    'present', 'shows', 'showed', 'the'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac8ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_statements = [\n",
    "    '', ' ', 'clinic', 'clinical',\n",
    "    'dx clinical', 'discussed dr',   \n",
    "    'findings discussed dr',\n",
    "    'first_name last_name', 'intubated', \n",
    "    'intubation', 'patient rotated',\n",
    "    'xr chest ap portable',\n",
    "    'this exam was dictated at', '____'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2821e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictation = 'this exam was dictated at this_hospital'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63177669",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ards_indicators'\n",
    "verbose = True\n",
    "indicator_words = curate_indicator_word_list(\n",
    "    filename,\n",
    "    targeted_stemming,\n",
    "    verbose\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a5706a4",
   "metadata": {},
   "source": [
    "## Code cell performing report/text segmentation: CXRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20786de",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "for record in cxr_list:\n",
    "        note = remove_easy_sections(\n",
    "                record['cxr_text'],\n",
    "                section_order[cohort.lower()]\n",
    "                )\n",
    "        if verbose: print(note, '\\n')\n",
    "        \n",
    "        new_report = note.split('.')\n",
    "        if verbose: print(f\"Note split---- {new_report}---\\n\")\n",
    "            \n",
    "        new_report = handle_subsection_titles(new_report)\n",
    "        if verbose: print(f\"++Split by :---- {new_report}---\\n\")\n",
    "            \n",
    "        new_report = remove_lines_on_other_organs(\n",
    "                new_report,\n",
    "                exclusion_set\n",
    "                )\n",
    "        if verbose: print(f\"++Exclusions---- {new_report}---\\n\")\n",
    "            \n",
    "        new_report = stem_indicator_words(\n",
    "                new_report,\n",
    "                targeted_stemming\n",
    "                )\n",
    "        if verbose: print(f\"++Stem Indic---- {new_report}---\\n\")\n",
    "            \n",
    "        new_report = remove_stopwords(\n",
    "                new_report,\n",
    "                complex_stopwords,\n",
    "                simple_stopwords\n",
    "                )\n",
    "        if verbose: print(f\"++Rem Stopw---- {new_report}---\\n\")\n",
    "            \n",
    "        new_report = remove_sections_n_duplicate_lines(new_report)\n",
    "        if verbose: print(f\"++Rem Dupli---- {new_report}---\\n\")\n",
    "            \n",
    "        new_report = refine_cleaning(\n",
    "                new_report,\n",
    "                useless_statements\n",
    "                )\n",
    "        if verbose: print(f\"++Clean---- {new_report}---\")\n",
    "            \n",
    "        new_report = remove_dictation(\n",
    "                new_report,\n",
    "                dictation,\n",
    "                verbose\n",
    "                )\n",
    "        if verbose: print(f\"++Dictation---- {new_report}---\")\n",
    "            \n",
    "        record['seg_cxr_text'] = deepcopy(new_report)\n",
    "        if verbose: print(record['seg_cxr_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48bf7036",
   "metadata": {},
   "source": [
    "## Annotated attending physician notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe2b7077",
   "metadata": {},
   "source": [
    "#### First, extract surrounding text mentioning risk factors. It currently extracts a 200-character window from the mention of a risk factor (100 before, and 100 after). If note isn't that long, it takes whatever it can take from the note (i.e. from beginning to end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46bb23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the relevant part of the notes mentioning the risk factors. Doing this now for all notes\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='pneumonia'\n",
    "    )\n",
    "\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='chf'\n",
    "    )\n",
    "\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='aspiration'\n",
    "    )\n",
    "\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='sepsis'\n",
    "    )\n",
    "\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='shock'\n",
    "    )\n",
    "\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='cardiac_arrest'\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f68f0e2f",
   "metadata": {},
   "source": [
    "## Saving segmented files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f7912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV files\n",
    "savepath = Path.cwd() / basedir / 'Analysis_Data' / 'MIMIC_III' / 'labeled_subset'\n",
    "\n",
    "cxr = pd.DataFrame(cxr_list)\n",
    "notes = pd.DataFrame(notes_list)\n",
    "\n",
    "pf.to_csv(savepath / \"pf_ratio.csv\", index=False)\n",
    "\n",
    "if peep is not None:\n",
    "    peep.to_csv(savepath / \"peep.csv\", index=False)\n",
    "    \n",
    "cxr.to_csv(savepath / \"cxr.csv\", index=False)\n",
    "notes.to_csv(savepath / \"attending_notes.csv\", index=False)\n",
    "echo.to_csv(savepath / \"echo_reports.csv\", index=False)\n",
    "bnp.to_csv(savepath / \"bnp.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a3d03936b2a91ffdc78963cfe8def908d987ac66206affb46df652cfa0d16791"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
