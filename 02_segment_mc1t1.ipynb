{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f5dc7e",
   "metadata": {},
   "source": [
    "# This notebook segments (i.e., breaks reports into key statements/words) tables containing report data for MC1-T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a56337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom imports\n",
    "from src.segmentation_tools import (\n",
    "    curate_indicator_word_list,\n",
    "    remove_easy_sections,\n",
    "    handle_subsection_titles,\n",
    "    remove_lines_on_other_organs,\n",
    "    stem_indicator_words,\n",
    "    remove_stopwords,\n",
    "    remove_sections_n_duplicate_lines,\n",
    "    refine_cleaning,\n",
    "    remove_dictation,\n",
    "    extract_surroundings_of_risk_factor_and_process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a2bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom display of tables for easier inspection\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60369573",
   "metadata": {},
   "source": [
    "This notebook aims to process datasets from MC1-T1 for downstream automated ARDS/control adjudication.  \n",
    "Specifically, this notebook follows steps highlighted in bold-face:  \n",
    "- File I/O, which depends on a specs file.\n",
    "- Standard preprocessing, in which column names are standardized.  \n",
    "- Hospital-specific processing, which is temporarily custom-made.  \n",
    "- Anonymization. It follows two substeps:  \n",
    "    + Anonymizing patient/encounter IDs and datetime columns for all tables.\n",
    "    + Anonymizing text-based tables (chest X-ray reports, attending notes, ECHO reports). \n",
    "- **Segmentation of text-based tables.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82826cd1",
   "metadata": {},
   "source": [
    "## Read in the tables and converting the text-based ones into list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5addbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = Path(\"..\")\n",
    "anonymized_location = basedir / 'Anonymized_data'\n",
    "cohort = 'mc1_t1'\n",
    "path = anonymized_location / cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba097aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = pd.read_csv(path / \"pf_ratio.csv\")\n",
    "pf['pf_ratio_timestamp'] = pd.to_timedelta(pf['pf_ratio_timestamp'])\n",
    "pf['vent_start_timestamp'] = pd.to_timedelta(pf['vent_start_timestamp'])\n",
    "\n",
    "try:\n",
    "    peep = pd.read_csv(path / \"peep.csv\")\n",
    "    peep['peep_timestamp'] = pd.to_timedelta(peep['peep_timestamp'])\n",
    "except FileNotFoundError:\n",
    "    peep = None\n",
    "    print(\"This dataset doesn't seem to have peep separately specified.\")\n",
    "\n",
    "cxr = pd.read_csv(path / \"cxr.csv\")\n",
    "cxr['cxr_timestamp'] = pd.to_timedelta(cxr['cxr_timestamp'])\n",
    "\n",
    "notes = pd.read_csv(path / \"attending_notes.csv\")\n",
    "notes['notes_timestamp'] = pd.to_timedelta(notes['notes_timestamp'])\n",
    "\n",
    "notes_annot = pd.read_csv(path / \"attending_notes_annotated.csv\")\n",
    "notes_annot['notes_timestamp'] = pd.to_timedelta(notes_annot['notes_timestamp'])\n",
    "\n",
    "echo = pd.read_csv(path / \"echo_reports.csv\")\n",
    "echo['echo_timestamp'] = pd.to_timedelta(echo['echo_timestamp'])\n",
    "\n",
    "bnp = pd.read_csv(path / \"bnp.csv\")\n",
    "bnp['bnp_timestamp'] = pd.to_timedelta(bnp['bnp_timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82caad5d",
   "metadata": {},
   "source": [
    "## Defining parameters for segmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd93c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does it actually matter listing different sections for the cohorts?\n",
    "# The pattern of True/False is the same, but with new sections.\n",
    "# Might as well have just one list of tuples with comprehensive sections\n",
    "# found across every cohort. If a section is not in the CXRs of one\n",
    "# particular cohort, it would simply not match, right?\n",
    "section_order = {}\n",
    "\n",
    "section_order['mc1_t1'] = [\n",
    "    ('result:', True),\n",
    "    ('study:', False),\n",
    "    ('procedure:', False),\n",
    "    ('indication:', False),\n",
    "    ('technique:', False),\n",
    "    ('history:', False),\n",
    "    ('exam:', False),\n",
    "    ('comparison:', False),\n",
    "    ('finding_conclusion:', True),\n",
    "    ('finding:', True),\n",
    "    ('impression:', True),\n",
    "    ('conclusion:', True)\n",
    "    ]\n",
    "\n",
    "section_order['mc1_t2'] = [\n",
    "    ('procedure:', False),\n",
    "    ('indication:', False),\n",
    "    ('technique:', False),\n",
    "    ('history:', False),\n",
    "    ('exam:', False),\n",
    "    ('comparison:', False),\n",
    "    ('finding_conclusion:', True),\n",
    "    ('finding:', True),\n",
    "    ('impression:', True),\n",
    "    ('conclusion:', True)\n",
    "    ]\n",
    "\n",
    "section_order['mc2_t3'] = [\n",
    "    ('procedure:', False),\n",
    "    ('indication:', False),\n",
    "    ('technique:', False),\n",
    "    ('history:', False),\n",
    "    ('exam:', False),\n",
    "    ('comparison:', False),\n",
    "    ('finding_conclusion:', True),\n",
    "    ('finding:', True),\n",
    "    ('impression:', True),\n",
    "    ('conclusion:', True)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caea9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words listed in exclusion_set mark statements in the report that do not address the lungs.\n",
    "#\n",
    "# Note: Curt and Cathy recommended removing 'hila', 'hilar' from this list.\n",
    "#\n",
    "# They suggested that it may also be appropriate to remove 'venous'\n",
    "# but it requires further consideration.\n",
    "\n",
    "exclusion_set = {\n",
    "    'adenopathy', 'artery', 'aortic', 'atria',\n",
    "    'biliary', 'bowel', 'bones', 'cabg', 'carina',\n",
    "    'cardiac', 'cardiomegaly', 'catheter', 'chest',\n",
    "    'cirrhosis', 'devices', 'drain', 'drains', 'ett',\n",
    "    'gallbladder', 'heart', 'hearts', 'hydronephrosis',\n",
    "    'kidney', 'line', 'lines', 'liver', 'lymph',\n",
    "    'mediastinal', 'mediastinum', 'myeloma', 'picc',\n",
    "    'pneumomediastinum', 'spine', 'spleen',\n",
    "    'support_devices', 'tube', 'tubes', 'tubes_devices',\n",
    "    'vasculature', 'vein', 'vena', 'venous', 'ventric',\n",
    "    'wire', 'wires'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62996367",
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_stemming = {\n",
    "    'bilaterally': 'bilateral',\n",
    "    'infiltrates': 'infiltrate',\n",
    "    'inhalational': 'inhalation',\n",
    "    'opacities': 'opacity',\n",
    "    'angles': 'angle',\n",
    "    'effusions': 'effusion',\n",
    "    'patches': 'patch',\n",
    "    'patchy': 'patch',\n",
    "    'spaces': 'space',\n",
    "    'traces': 'trace'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2813e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_stopwords = [\n",
    "    'there is ',\n",
    "    'there are ',\n",
    "    'there has been ',\n",
    "    'at this time'\n",
    "    ]\n",
    "\n",
    "simple_stopwords = [\n",
    "    'a', 'an', 'are', 'demonstrate',\n",
    "    'demonstrated', 'is', 'noted',\n",
    "    'present', 'shows', 'showed', 'the'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_statements = [\n",
    "    '', ' ', 'clinic', 'clinical',\n",
    "    'dx clinical', 'discussed dr',\n",
    "    'findings discussed dr',\n",
    "    'first_name last_name', 'intubated',\n",
    "    'intubation', 'patient rotated',\n",
    "    'xr chest ap portable',\n",
    "    'this exam was dictated at', '____'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26086f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictation = 'this exam was dictated at this_hospital'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a5578",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ards_indicators'\n",
    "verbose = False\n",
    "indicator_words = curate_indicator_word_list(\n",
    "    filename,\n",
    "    targeted_stemming,\n",
    "    verbose\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d5584",
   "metadata": {},
   "source": [
    "## Code cell performing report/text segmentation: CXRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3792f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cxr_list = cxr.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80abd14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "for record in cxr_list:\n",
    "    note = remove_easy_sections(\n",
    "        record['cxr_findings'],\n",
    "        section_order[cohort.strip('/')]\n",
    "        )\n",
    "    if verbose: print(note, '\\n')\n",
    "        \n",
    "    new_report = note.split('.')\n",
    "    if verbose: print(f\"Note split---- {new_report}---\\n\")\n",
    "            \n",
    "    new_report = handle_subsection_titles(new_report)\n",
    "    if verbose: print(f\"++Split by :---- {new_report}---\\n\")\n",
    "            \n",
    "    new_report = remove_lines_on_other_organs(\n",
    "        new_report,\n",
    "        exclusion_set\n",
    "        )\n",
    "    if verbose: print(f\"++Exclusions---- {new_report}---\\n\")\n",
    "            \n",
    "    new_report = stem_indicator_words(\n",
    "        new_report,\n",
    "        targeted_stemming\n",
    "        )\n",
    "    if verbose: print(f\"++Stem Indic---- {new_report}---\\n\")\n",
    "            \n",
    "    new_report = remove_stopwords(\n",
    "        new_report,\n",
    "        complex_stopwords,\n",
    "        simple_stopwords\n",
    "        )\n",
    "    if verbose: print(f\"++Rem Stopw---- {new_report}---\\n\")\n",
    "            \n",
    "    new_report = remove_sections_n_duplicate_lines(new_report)\n",
    "    if verbose: print(f\"++Rem Dupli---- {new_report}---\\n\")\n",
    "            \n",
    "    new_report = refine_cleaning(\n",
    "        new_report,\n",
    "        useless_statements\n",
    "        )\n",
    "    if verbose: print(f\"++Clean---- {new_report}---\")\n",
    "            \n",
    "    new_report = remove_dictation(\n",
    "        new_report,\n",
    "        dictation,\n",
    "        verbose\n",
    "        )\n",
    "    if verbose: print(f\"++Dictation---- {new_report}---\")\n",
    "            \n",
    "    record['seg_cxr_text'] = deepcopy(new_report)\n",
    "    if verbose: print(record['seg_cxr_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa46ea8",
   "metadata": {},
   "source": [
    "## Annotated attending physician notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7694dfec",
   "metadata": {},
   "source": [
    "#### First, extract surrounding text mentioning risk factors. It currently extracts a 200-character window from the mention of a risk factor (100 before, and 100 after). If note isn't that long, it takes whatever it can take from the note (i.e. from beginning to end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed09d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making those list of dict. The whole notes table plus SW notes are also being processed.\n",
    "notes_list = notes.to_dict(orient='records')\n",
    "notes_annot_list = notes_annot.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac9785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the relevant part of the notes mentioning the risk factors. Doing this now for all notes\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='pneumonia'\n",
    "    )\n",
    "\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='chf'\n",
    "    )\n",
    "\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='aspiration'\n",
    "    )\n",
    "\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='sepsis'\n",
    "    )\n",
    "\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='shock'\n",
    "    )\n",
    "\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='cardiac_arrest'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f820031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the relevant part of the notes mentioning the risk factors. Now doing this for training dataset (SW notes)\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_annot_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='pneumonia'\n",
    "    )\n",
    "\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_annot_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='chf'\n",
    "    )\n",
    "\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_annot_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='aspiration'\n",
    "    )\n",
    "\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_annot_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='sepsis'\n",
    "    )\n",
    "\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_annot_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='shock'\n",
    "    )\n",
    "\n",
    "extract_surroundings_of_risk_factor_and_process(\n",
    "    notes_annot_list,\n",
    "    text_field='notes_text',\n",
    "    add_column_name='cardiac_arrest'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c511749",
   "metadata": {},
   "source": [
    "## Saving segmented files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689644da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV files\n",
    "savepath = Path.cwd() / basedir / 'Analysis_Data' / 'mc1_t1'\n",
    "ml_savepath = Path.cwd() / basedir / 'Analysis_Data' / 'train_ML'\n",
    "\n",
    "cxr = pd.DataFrame(cxr_list)\n",
    "notes = pd.DataFrame(notes_list)\n",
    "notes_annot = pd.DataFrame(notes_annot_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f474c0",
   "metadata": {},
   "source": [
    "Uncomment if intending to overwrite files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.to_csv(savepath / \"pf_ratio.csv\", index=False)\n",
    "if peep is not None:\n",
    "    peep.to_csv(savepath / \"peep.csv\", index=False)\n",
    "    \n",
    "cxr.to_csv(savepath / \"cxr.csv\", index=False)\n",
    "notes.to_csv(savepath / \"attending_notes.csv\", index=False)\n",
    "notes_annot.to_csv(savepath / \"attending_notes_annotated.csv\", index=False)\n",
    "notes_annot.to_csv(ml_savepath / \"attending_notes_annotated.csv\", index=False)\n",
    "echo.to_csv(savepath / \"echo_reports.csv\", index=False)\n",
    "bnp.to_csv(savepath / \"bnp.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('ards_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5e25a126b1dea40460463543904200c94dc75bd51cc3498d0b8d6d8003382f35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
