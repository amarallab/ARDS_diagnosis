{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom imports\n",
    "from src.diagnosis_tools import (\n",
    "    mark_hypoxemic_episodes,\n",
    "    mark_abnormal_cxr,\n",
    "    mark_cxr_within_48h_of_post_vent_hypoxemia,\n",
    "    mark_note_within_7d,\n",
    "    mark_notes_with_ml,\n",
    "    text_match_risk_factors,\n",
    "    diagnose_or_exclude_encounters,\n",
    "    flag_echos\n",
    ")\n",
    "import src.plots as plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plotting params\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.style.reload_library()\n",
    "rcparams = plots.stdrcparams1()\n",
    "mpl.rcParams.update(rcparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = Path(\"..\")\n",
    "training_location = basedir / 'Analysis_Data' / 'train_ML'\n",
    "path = basedir / 'Analysis_Data' / 'MIMIC_III' / 'labeled_subset'\n",
    "figure_path = basedir / \"Figures\"\n",
    "raw_path = basedir / 'Raw_Data' / 'MIMIC_III' / 'labeled_subset'\n",
    "feihong_path = basedir / 'for_Curt_MIMIC_III'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = pd.read_csv(path / \"pf_ratio.csv\")\n",
    "pf['pf_ratio_timestamp'] = pd.to_datetime(pf['pf_ratio_timestamp'])\n",
    "pf['vent_start_timestamp'] = pd.to_datetime(pf['vent_start_timestamp'])\n",
    "\n",
    "try:\n",
    "    peep = pd.read_csv(path / \"peep.csv\")\n",
    "    peep['peep_timestamp'] = pd.to_datetime(peep['peep_timestamp'])\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    peep = None\n",
    "    print(\"This dataset doesn't seem to have peep separately specified.\")\n",
    "\n",
    "cxr = pd.read_csv(path / \"cxr.csv\")\n",
    "cxr['cxr_timestamp'] = pd.to_datetime(cxr['cxr_timestamp'])\n",
    "\n",
    "notes = pd.read_csv(path / \"attending_notes.csv\")\n",
    "notes['notes_timestamp'] = pd.to_datetime(notes['notes_timestamp'])\n",
    "\n",
    "echo = pd.read_csv(path / \"echo_reports.csv\")\n",
    "echo['echo_timestamp'] = pd.to_datetime(echo['echo_timestamp'])\n",
    "\n",
    "bnp = pd.read_csv(path / \"bnp.csv\")\n",
    "bnp['bnp_timestamp'] = pd.to_datetime(bnp['bnp_timestamp'])\n",
    "\n",
    "final_numbers = pd.read_excel(raw_path / \"ARDS_Criteria_Curt.xlsx\")\n",
    "\n",
    "final_numbers.rename(\n",
    "    columns={\n",
    "        'HADM_ID': 'encounter_id',\n",
    "        'FINAL ARDS CURT (1=YES)': 'clinician_diagnosed'\n",
    "        },\n",
    "    inplace=True\n",
    "    )\n",
    "        \n",
    "encounters_by_Curt = final_numbers[[\n",
    "    'encounter_id',\n",
    "    'clinician_diagnosed']] \\\n",
    "        .drop_duplicates() \\\n",
    "        .groupby('encounter_id')['clinician_diagnosed'] \\\n",
    "        .sum().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_encounters = pf['encounter_id'].drop_duplicates()\n",
    "\n",
    "if peep is not None:\n",
    "    peep_encounters = peep['encounter_id'].drop_duplicates()\n",
    "else:\n",
    "    peep_encounters = None\n",
    "    \n",
    "cxr_encounters = cxr['encounter_id'].drop_duplicates()\n",
    "notes_encounters = notes['encounter_id'].drop_duplicates()\n",
    "echo_encounters = echo['encounter_id'].drop_duplicates()\n",
    "bnp_encounters = bnp['encounter_id'].drop_duplicates()\n",
    "\n",
    "if peep is None:\n",
    "    total_encounters = pd.merge(pf_encounters, cxr_encounters, how='outer').drop_duplicates()\n",
    "    print(f\"Patient encounters with PF ratios or CXRs: {len(total_encounters)}\")\n",
    "else:\n",
    "    total_encounters = pd.merge(pf_encounters, peep_encounters, how='outer').drop_duplicates()\n",
    "    total_encounters = pd.merge(total_encounters, cxr_encounters, how='outer').drop_duplicates()\n",
    "    print(f\"Patient encounters with PF ratios, PEEP, or CXRs: {len(total_encounters)}\")\n",
    "    \n",
    "total_encounters = pd.merge(total_encounters, notes_encounters, how='outer').drop_duplicates()\n",
    "total_encounters = pd.merge(total_encounters, echo_encounters, how='outer').drop_duplicates()\n",
    "total_encounters = pd.merge(total_encounters, bnp_encounters, how='outer').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will be dictionaries whose keys will become the column names for the flags\n",
    "# and the lists will be the regex patterns to search for\n",
    "\n",
    "# (?i) is to inactivate case-sensitivity\n",
    "# (?:) is to indicate that contents inside a parenthesis shouldn't be read as a \"capturing group\"\n",
    "# Default behavior of () is to consider it a capturing group\n",
    "echo_prefix = {'lvef': ['(?i)lv\\s+ejection\\s+fraction',\n",
    "                        '(?i)left\\s+ventricular\\s+ejection\\s+fraction',\n",
    "                        '(?i)lvef',\n",
    "                        '(?i)left\\s+ventricular\\s+ef',\n",
    "                        '(?i)lvef\\s+is',\n",
    "                        '(?i)left\\s+ventricle\\s+ejection\\s+fraction\\s+is',\n",
    "                        '(?i)lv\\s+ejection\\s+fraction\\s+is'],\n",
    "               \n",
    "               # Match \"cardiopulmonary bypass\" ensuring at least one whitespace character between those words\n",
    "              'cp_bypass': ['(?i)cardiopulmonary\\s+bypass'],\n",
    "              \n",
    "              'la_dimension': ['(?i)la\\s+diameter',\n",
    "                               '(?i)la\\s+dimension'],\n",
    "\n",
    "              'la_volume_index': ['(?i)la\\s+volume',\n",
    "                                  '(?i)LA\\s+Vol\\s+BP\\s+A/L\\s+Index'],\n",
    "              \n",
    "              'lv_hypertrophy': ['(?i)(?<!borderline\\s)(?:left\\s+ventricular|lv|lv\\s+concentric)\\s*hypertrophy',\n",
    "                                 '(?i)(?<!borderline\\s)LVH'],\n",
    "              \n",
    "              'diastolic_dysfunction': ['(?i)(grade\\s*ii)',\n",
    "                                        '(?i)(grade\\s*iii)']}\n",
    "\n",
    "echo_suffix = {'lvef': '\\D{0,20}(\\d{1,3}|\\d{1,2}\\s*-\\s*\\d{1,3})-{0,1}\\s*%', # Sample matches: 45%, 45 %, 45-55%, 45 - 55 %, 45- 100%, 45- %\n",
    "               'cp_bypass': '(?!\\s*N\\/A|\\s*Patient\\s+was\\s+not\\s+placed\\s+on\\s+cardiopulmonary\\s+bypass|\\s*NA)',  # Don't match if N/A or Patient wasn't placed on CPB\n",
    "               'la_dimension': '\\D{0,25}(\\d\\.\\s*\\d)\\s*(?:cm|centimeter)', # Sample matches: 2.7cm, 2.7 cm, 2.7   centimeter\n",
    "               \n",
    "                # Match anything until \"ml\" appears once or never, then match anything until the number of interest appears\n",
    "                # followed by either ml/m or ml per square meter\n",
    "               'la_volume_index': '.*?(?:ml)?.*?(\\d+\\.\\s*\\d+)\\s+(?:(?=ml\\/m)|(?=ml\\s+per\\s+square\\s+meter))',\n",
    "               'lv_hypertrophy': '',\n",
    "               # Matches anything, either never or up to 30 characters, then an arbitrary number of white spaces,\n",
    "               # as long as \"diastolic dysfunction\" immediately follows.\n",
    "               'diastolic_dysfunction': '.{0,30}\\s*?(?=diastolic\\s+dysfunction)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Flag ECHO reports (we can do this now since it is independent of previous steps. Plus, it saves runtime)\n",
    "echo = flag_echos(echo, echo_prefix, echo_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Adjudicate hypoxemia\n",
    "pf, hypox_df = mark_hypoxemic_episodes(pf, peep, 'encounter_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = training_location / 'cxr_whole_training_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cxr_threshs = np.linspace(0, 1, 1001)\n",
    "notes_threshs = np.linspace(0, 1, 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for cxr_thresh in tqdm(cxr_threshs):\n",
    "    \n",
    "    # 2a. Adjudicate bilateral infiltrates with ML\n",
    "    cxr = mark_abnormal_cxr(\n",
    "        cxr,\n",
    "        train_data,\n",
    "        train_col=['segmented_report', 'score'],\n",
    "        test_label_col='curt_bl_infiltrates_(1=yes)',\n",
    "        thresholding=\"custom\",\n",
    "        custom_threshold=cxr_thresh\n",
    "        )\n",
    "    \n",
    "    # 2b. Flag CXRs that are within 48h of hypoxemia, and hypoxemia events that are post intubation\n",
    "    cxr, hypox_pred_abn_cxr_48h = mark_cxr_within_48h_of_post_vent_hypoxemia(\n",
    "        hypox_df,\n",
    "        cxr,\n",
    "        'encounter_id',\n",
    "        'cxr_timestamp'\n",
    "        )\n",
    "    \n",
    "    # 3a. Flag notes that are within 7d of hypoxemia or bilateral infiltrates (whichever is latest)\n",
    "    notes = mark_note_within_7d(notes, hypox_df, hypox_pred_abn_cxr_48h, 'encounter_id', 'cxr_timestamp')\n",
    "    \n",
    "    for notes_thresh in tqdm(notes_threshs):\n",
    "        \n",
    "        # 3b. Adjudicate pneumonia with ML\n",
    "        notes = mark_notes_with_ml(\n",
    "            notes,\n",
    "            training_location,\n",
    "            train_col=['seg_pneumonia', 'pneumonia_sw'],\n",
    "            test_label_col='curt_pneumonia_(1=yes)',\n",
    "            thresholding=\"custom\",\n",
    "            custom_threshold=notes_thresh\n",
    "            )\n",
    "        \n",
    "        # 3c. Adjudicate risk factors or heart failure with text matching\n",
    "        notes = text_match_risk_factors(notes)\n",
    "        \n",
    "        # 4. Make diagnosis decisions based on above flags\n",
    "        notes, diagnosed, excluded, for_objective_assessment = diagnose_or_exclude_encounters(\n",
    "            notes,\n",
    "            hypox_pred_abn_cxr_48h,\n",
    "            'encounter_id'\n",
    "            )\n",
    "        \n",
    "        # Encounters without evidence of risk factors or heart failure on notes enter the next stages:\n",
    "        # 5a. BNP > 100 rule out\n",
    "        a = bnp['bnp_value'] > 100\n",
    "        encounters_with_bnp_greater_than_100 = list(bnp.loc[a, 'encounter_id'].unique())\n",
    "\n",
    "        j = for_objective_assessment['encounter_id'].isin(encounters_with_bnp_greater_than_100)\n",
    "        remaining_after_bnp = for_objective_assessment.loc[~j]\n",
    "        \n",
    "        # 5b. LVEF < 40% rule out\n",
    "        b = echo['lvef_value'] < 40\n",
    "        encounters_with_lvef_smaller_than_40 = list(echo.loc[b, 'encounter_id'].unique())\n",
    "\n",
    "        j = remaining_after_bnp['encounter_id'].isin(encounters_with_lvef_smaller_than_40)\n",
    "        remaining_after_lvef = remaining_after_bnp.loc[~j]\n",
    "        \n",
    "        # 5c. Cardiopulmonary bypass rule out\n",
    "        cpb = echo['cp_bypass_value'].notnull()\n",
    "        encounters_with_cardiopulmonary_bypass = list(echo.loc[cpb, 'encounter_id'].unique())\n",
    "\n",
    "        j = remaining_after_lvef['encounter_id'].isin(encounters_with_cardiopulmonary_bypass)\n",
    "        remaining_after_cpb = remaining_after_lvef.loc[~j]\n",
    "        \n",
    "        # 5d. Two out of three: (LA dim > 4 cm or LA volume index > 28 ml/m^2), LV hypertrophy, diastolic dysfunction\n",
    "        la_dim = echo['la_dimension_value'] > 4\n",
    "        la_vol_idx = echo['la_volume_index_value'] > 28\n",
    "        echo.loc[:, 'la_enlargement_bool'] = (la_dim | la_vol_idx).astype(int)\n",
    "        echo.loc[:, 'lv_hypertrophy_bool'] = echo['lv_hypertrophy_value'].notnull().astype(int)\n",
    "        echo.loc[:, 'diastolic_dysfunction_bool'] = echo['diastolic_dysfunction_value'].notnull().astype(int)\n",
    "\n",
    "        echo['additional_criteria_count'] = echo['la_enlargement_bool'] + echo['lv_hypertrophy_bool'] + echo['diastolic_dysfunction_bool']\n",
    "        \n",
    "        add_crit = echo['additional_criteria_count'] > 1\n",
    "        encounters_with_additional_criteria = list(echo.loc[add_crit, 'encounter_id'].unique())\n",
    "\n",
    "        j = remaining_after_cpb['encounter_id'].isin(encounters_with_additional_criteria)\n",
    "        remaining_after_additional_criteria = remaining_after_cpb.loc[~j]\n",
    "        \n",
    "        # Collecting diagnosed encounters\n",
    "        encounters_diagnosed_by_pipeline = pd.merge(\n",
    "            diagnosed['encounter_id'].drop_duplicates(),\n",
    "            remaining_after_additional_criteria['encounter_id'].drop_duplicates(),\n",
    "            how='outer'\n",
    "            ).drop_duplicates()\n",
    "        \n",
    "        # Creating encounters table\n",
    "        encounter_summary = pd.merge(\n",
    "            total_encounters,\n",
    "            encounters_diagnosed_by_pipeline,\n",
    "            how='outer',\n",
    "            indicator=True\n",
    "            )\n",
    "        \n",
    "        encounter_summary = encounter_summary.replace(\n",
    "            to_replace={\n",
    "                '_merge': {\n",
    "                    \"left_only\": 'No',\n",
    "                    \"both\": 'Yes'\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "\n",
    "        encounter_summary = encounter_summary.rename(columns={'_merge': \"pipeline_diagnosed\"})\n",
    "                \n",
    "        # Adding encounters diagnosed by Curt\n",
    "        encounter_summary = pd.merge(encounter_summary, encounters_by_Curt, how='outer')\n",
    "        encounter_summary = encounter_summary.replace(to_replace={'clinician_diagnosed': {0: \"No\", 1: \"Yes\"}})\n",
    "        \n",
    "        y_true = encounter_summary['clinician_diagnosed']\n",
    "        y_pred = encounter_summary['pipeline_diagnosed']\n",
    "        \n",
    "        cf = confusion_matrix(y_true, y_pred)\n",
    "        TN = cf[0,0]\n",
    "        FP = cf[0,1]\n",
    "        FN = cf[1,0]\n",
    "        TP = cf[1,1]\n",
    "        \n",
    "        # Collect thresholds and metrics for a potential table\n",
    "        results.append({\n",
    "            'cxr_threshold': cxr_thresh,\n",
    "            'notes_threshold': notes_thresh,\n",
    "            'false_negative_rate': FN / (FN + TP),\n",
    "            'false_positive_rate': FP / (FP + TN),\n",
    "            'precision': precision_score(y_true, y_pred, pos_label='Yes'),\n",
    "            'negative_predictive_value': TN / (TN + FN),\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'f1': f1_score(y_true, y_pred, pos_label='Yes')\n",
    "            })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"youden_j\"] = (1 - results_df[\"false_negative_rate\"]) - results_df[\"false_positive_rate\"]\n",
    "results_df.to_csv(\"mimic_threshold_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.read_csv(\"mimic_threshold_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_youden = results_df['youden_j'] == results_df['youden_j'].max()\n",
    "max_f1 = results_df['f1'] == results_df['f1'].max()\n",
    "max_accuracy = results_df['accuracy'] == results_df['accuracy'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[max_accuracy].sort_values('notes_threshold', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(results_df['notes_threshold'], results_df['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[max_f1].sort_values('cxr_threshold', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(results_df['notes_threshold'], results_df['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[max_youden].sort_values('cxr_threshold', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(results_df['notes_threshold'], results_df['youden_j'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom display of tables for easier inspection\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[max_youden & max_f1 & max_accuracy].sort_values('notes_threshold', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[(results_df['cxr_threshold'] == 0.5) & (results_df['notes_threshold'] == 0.5)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
